# -*- coding: utf-8 -*-
import sys
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from llama_cpp import Llama
import os
import random
import time
from datetime import datetime
import logging
from pathlib import Path
import re
import json
from typing import Any, Dict, List, Optional, Union
import psutil
from dataclasses import dataclass
import unicodedata


# === –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨–°–ö–ò–ï –ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø –î–õ–Ø –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–ò ===
class SecurityError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    pass

class ConfigurationError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –æ—à–∏–±–æ–∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    pass


@dataclass
class GenerationMetrics:
    """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    tokens_generated: int
    latency_ms: float
    memory_peak_mb: float
    throughput_tokens_per_second: float
    temperature_used: float
    model_device: str
    

class EnhancedGenerationManager:
    """
    –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
    dynamic configuration –∏ performance monitoring
    """

    def __init__(self, base_config: Dict[str, Any], monitoring_config: Dict[str, Any]):
        self.base_generation_config = base_config
        self.monitoring = monitoring_config
        self.metrics_history = []

    def get_generation_params(self, task_type: str = "general", override_params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        - JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        - –¢–∏–ø–∞ –∑–∞–¥–∞—á–∏
        - Runtime overrides
        """
        # === –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ JSON ===
        params = self.base_generation_config.copy()

        # === Task-specific adjustments ===
        task_adjustments = self._get_task_specific_adjustments(task_type)
        params.update(task_adjustments)

        # === Runtime overrides (highest priority) ===
        if override_params:
            params.update(override_params)

        # === –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è ===
        return self._validate_and_normalize_params(params)

    def _get_task_specific_adjustments(self, task_type: str) -> Dict[str, Any]:
        """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∑–∞–¥–∞—á"""
        adjustments = {
            "code_generation": {
                "temperature": 0.3,  # –ë–æ–ª–µ–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Å—Ç–∏—á–Ω–æ –¥–ª—è –∫–æ–¥–∞
                "top_p": 0.9,
                "repetition_penalty": 1.2,
                "max_new_tokens": 1500
            },
            "code_analysis": {
                "temperature": 0.4,
                "top_p": 0.85,
                "max_new_tokens": 1200
            },
            "code_fixing": {
                "temperature": 0.2,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
                "top_p": 0.8,
                "max_new_tokens": 1200
            },
            "code_explanation": {
                "temperature": 0.6,  # –ë–æ–ª–µ–µ —Ç–≤–æ—Ä—á–µ—Å–∫–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
                "top_p": 0.95,
                "max_new_tokens": 1500
            },
            "free_mode": {
                "temperature": 0.7,
                "top_p": 0.95,
                "max_new_tokens": 1024
            }
        }
        return adjustments.get(task_type, {})

    def _validate_and_normalize_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        # === –í–∞–ª–∏–¥–∞—Ü–∏—è ranges ===
        params["temperature"] = max(0.1, min(2.0, params.get("temperature", 0.7)))
        params["top_p"] = max(0.1, min(1.0, params.get("top_p", 0.95)))
        params["top_k"] = max(1, min(200, params.get("top_k", 50)))
        params["max_new_tokens"] = max(50, min(4096, params.get("max_new_tokens", 1200)))

        # === –û–±—Ä–∞–±–æ—Ç–∫–∞ null –∑–Ω–∞—á–µ–Ω–∏–π ===
        if params.get("pad_token_id") is None:
            params.pop("pad_token_id", None)

        return params

    def record_metrics(self, metrics: GenerationMetrics):
        """–ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫ –¥–ª—è monitoring –∏ optimization"""
        if self.monitoring.get("performance_tracking", True):
            self.metrics_history.append(metrics)

            # === Retention policy ===
            max_history = self.monitoring.get("metrics_retention", 1000)
            if len(self.metrics_history) > max_history:
                self.metrics_history = self.metrics_history[-max_history:]


# === –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç–∏–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ ===
AIDA_THEME = "matrix"


class EnhancedCodeAssistant:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∞–Ω–∞–ª–∏–∑–∞, –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–¥–∞."""
    def __init__(self, config_path="config.json"):
        # === –ë–ï–ó–û–ü–ê–°–ù–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–£–¢–ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò ===
        if config_path is None:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è -> –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª -> –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
            config_path = os.environ.get('AIDA_CONFIG_PATH')
            if config_path is None:
                script_dir = Path(__file__).parent.absolute()
                default_config = script_dir / "config.json"
                if default_config.exists():
                    config_path = str(default_config)
                else:
                    raise FileNotFoundError(
                        f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω. –û–∂–∏–¥–∞–µ—Ç—Å—è: {default_config}\n"
                        "C–æ–∑–¥–∞–π—Ç–µ config.json –≤ —Ç–æ–º –∂–µ –∫–∞—Ç–∞–ª–æ–≥–µ, —á—Ç–æ –∏ —Å–∫—Ä–∏–ø—Ç, –∏–ª–∏"
                        "—É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é —Å—Ä–µ–¥—ã AIDA_CONFIG_PATH."
                    )
        
        self.config_path = Path(config_path).resolve()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—É—Ç–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if not self._validate_config_path_security(self.config_path):
            raise SecurityError(f"–ù–µ–≤–µ—Ä–Ω—ã–π –ø—É—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {self.config_path}")

        self.model = None
        self.tokenizer = None
        self.is_gguf = False
        self.conversation_history = []
        self.context_window = 5  # –£–≤–µ–ª–∏—á–µ–Ω–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        self.max_history = 100
        
        # === –°–∞–º–æ—Å–æ–∑–Ω–∞–Ω–∏–µ –ê–∏–¥—ã ===
        self.name = "–ê–∏–¥–∞"
        self.personality = {
            "friendly": True,
            "helpful": True,
            "curious": True,
            "supportive": True
        }
        self.user_name = None
        self.session_start = datetime.now()

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞ –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–µ
        self.logger = None

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        try:
            self.load_config()
            self.setup_logging()
        except Exception as init_error:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {init_error}")
            print("‚ö†Ô∏è –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ fallback –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é...")
            # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è graceful degradation
            self._initialize_fallback_config()
            self._setup_fallback_logging()
            print("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–µ–∂–∏–º–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏")

        # –£–ª—É—á—à–µ–Ω–Ω—ã–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è DeepSeek
        self.system_prompts = {
            "code_generation": f"""–¢—ã {self.name} - –æ–ø—ã—Ç–Ω—ã–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å –≥–ª—É–±–æ–∫–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏.
–°–æ–∑–¥–∞–≤–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–π, —Ä–∞–±–æ—á–∏–π –∫–æ–¥ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.

–ü–†–ò–ù–¶–ò–ü–´:
- –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å —è–∑—ã–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
- –ü–∏—à–∏ —á–∏—Ç–∞–µ–º—ã–µ –∏–º–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏ —Ñ—É–Ω–∫—Ü–∏–π
- –î–æ–±–∞–≤–ª—è–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –°–ª–µ–¥—É–π –ª—É—á—à–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
- –ö–æ–¥ –¥–æ–ª–∂–µ–Ω –∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å—Å—è –∏ —Ä–∞–±–æ—Ç–∞—Ç—å –±–µ–∑ –æ—à–∏–±–æ–∫
- –ü—Ä–∏–º–µ–Ω—è–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.""",

            "code_analysis": f"""–¢—ã {self.name} - senior software architect —Å —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–æ–π –≤ code review.

–°–¢–†–£–ö–¢–£–†–ê –ê–ù–ê–õ–ò–ó–ê:
1. –°–ò–ù–¢–ê–ö–°–ò–°: –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—à–∏–±–æ–∫ –∫–æ–º–ø–∏–ª—è—Ü–∏–∏
2. –õ–û–ì–ò–ö–ê: –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
3. –ê–†–•–ò–¢–ï–ö–¢–£–†–ê: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∏ –¥–∏–∑–∞–π–Ω
4. –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
5. –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨: —É—è–∑–≤–∏–º–æ—Å—Ç–∏ –∏ —Ä–∏—Å–∫–∏
6. –°–¢–ò–õ–¨: —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è

–ü—Ä–æ–≤–æ–¥–∏ –≥–ª—É–±–æ–∫–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.""",

            "code_fixing": f"""–¢—ã {self.name} - expert debugging specialist –∏ refactoring engineer.

–ê–õ–ì–û–†–ò–¢–ú –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
1. –í—ã—è–≤–ª—è—é –≤—Å–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
2. –ò—Å–ø—Ä–∞–≤–ª—è—é –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
3. –û–ø—Ç–∏–º–∏–∑–∏—Ä—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
4. –£–ª—É—á—à–∞—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –∫–æ–¥–∞
5. –î–æ–±–∞–≤–ª—è—é –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
6. –ü—Ä–∏–≤–æ–∂—É –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è

–í–æ–∑–≤—Ä–∞—â–∞—é –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–¥ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏. –û—Ç–≤–µ—á–∞—é –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.""",

            "code_explanation": f"""–¢—ã {self.name} - senior technical educator –∏ software development mentor.

–ú–ï–¢–û–î–ò–ö–ê –û–ë–™–Ø–°–ù–ï–ù–ò–Ø:
1. –û–ë–ó–û–†: —á—Ç–æ –¥–µ–ª–∞–µ—Ç –ø—Ä–æ–≥—Ä–∞–º–º–∞ –≤ —Ü–µ–ª–æ–º
2. –°–¢–†–£–ö–¢–£–†–ê: —Ä–∞–∑–±–æ—Ä –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
3. –ê–õ–ì–û–†–ò–¢–ú: –ø–æ—à–∞–≥–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ª–æ–≥–∏–∫–∏
4. –î–ï–¢–ê–õ–ò: –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤
5. –ü–†–ò–ú–ï–ù–ï–ù–ò–ï: –≥–¥–µ –∏ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å

–û–±—ä—è—Å–Ω—è—é –¥–æ—Å—Ç—É–ø–Ω–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ. –û—Ç–≤–µ—á–∞—é –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."""
        }

        
    def _validate_config_path_security(self, config_path: Path) -> bool:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –ø—É—Ç–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏

        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
        - –°—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
        - Directory traversal –∞—Ç–∞–∫–∏
        - –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        - –ü—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
            if not config_path.exists():
                raise FileNotFoundError(f"Config file does not exist: {config_path}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ directory traversal –∞—Ç–∞–∫–∏
            config_path.resolve(strict=True)

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è —Ñ–∞–π–ª–∞
            if config_path.suffix.lower() != '.json':
                self.log_error("config_security", f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º–æ–µ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {config_path.suffix}")
                return False

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞
            if not os.access(config_path, os.R_OK):
                self.log_error("config_security", f"–ù–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ —á—Ç–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {config_path}")
                return False

            return True
        
        except Exception as e:
            if hasattr(self, 'logger') and self.logger:
                self.log_error("config_path_validation", f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞: {e}")
            else:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return False


    def _validate_local_model_path(self, local_path: str) -> Optional[Path]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø—É—Ç–∏ –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    
        Args:
            local_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            
        Returns:
            Path: –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
            
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
        - –°—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        - –ù–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
        - –ü—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞
        - –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –ø—É—Ç–∏
        """
        try:
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Ç–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π ~ –∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—É—Ç–µ–π
            model_path = Path(local_path).expanduser().resolve()

            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
            if not model_path.exists():
                self.log_error("model_validation", f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {model_path}")
                return None

            if not model_path.is_dir():
                self.log_error("model_validation", f"–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∫–∞—Ç–∞–ª–æ–≥–æ–º: {model_path}")
                return None

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ directory traversal
            try:
                model_path.resolve(strict=True)
            except (OSError, RuntimeError) as e:
                self.log_error("model_validation", f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—É—Ç–∏: {e}")
                return None

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
            required_files = ["config.json"]
            model_files = ["pytorch_model.bin", "model.safetensors"]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º config.json –º–æ–¥–µ–ª–∏
            if not (model_path / "config.json").exists():
                self.log_error("model_validation", f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç config.json –≤ {model_path}")
                return None

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏
            has_weights = any((model_path / f).exists() for f in model_files)
            if not has_weights:
                self.log_error("model_validation", f"–í–µ—Å–∞ –º–æ–¥–µ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {model_path}")
                return None

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞
            if not os.access(model_path, os.R_OK):
                self.log_error("model_validation", f"–ù–µ—Ç —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –Ω–∞ —á—Ç–µ–Ω–∏–µ –∫–∞—Ç–∞–ª–æ–≥–∞ –º–æ–¥–µ–ª–∏: {model_path}")
                return None

            self.log_system_event(f"–ü—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–≤–µ—Ä–µ–Ω: {model_path}")
            return model_path

        except Exception as e:
            self.log_error("model_path_validation", f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {local_path}: {e}")
            return None


    def load_config(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ JSON-—Ñ–∞–π–ª–∞"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
            self.model_info = config.get("model", {
                "name": "deepseek-ai/deepseek-coder-6.7b-instruct",
                "description": "DeepSeek Coder 6.7B Instruct",
                "size": "6.7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤",
                "context_size": 4096
            })

            model_config = config.get("model", {})
            if model_config.get("use_local", False):
                local_path = model_config.get("local_path", "")
                if not local_path:
                    raise ValueError("local_path –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —É–∫–∞–∑–∞–Ω, –µ—Å–ª–∏ use_local –∏–º–µ–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ true")
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
                validated_path = self._validate_local_model_path(local_path)
                if not validated_path:
                    if model_config.get("fallback_to_remote", True):
                        self.log_error("config_load", f"–ù–µ–≤–µ—Ä–Ω—ã–π local_path: {local_path}, –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ –∫ —É–¥–∞–ª–µ–Ω–Ω–æ–º—É —Ä–µ–∂–∏–º—É")
                        model_config["use_local"] = False
                    else:
                        raise FileNotFoundError(f"–ù–µ–≤–µ—Ä–Ω—ã–π –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {local_path}")
                else:
                    model_config["local_path"] = str(validated_path)
                    self.log_system_event(f"–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–π –ø—É—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {validated_path}")

            self.model_info = model_config

            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã
            settings = config.get("settings", {
                "device": "auto",
                "cache_dir": "./model_cache",
                "dtype": "bfloat16"
            })

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
            if settings["device"] == "auto":
                self.device = "cuda" if torch.cuda.is_available() else "cpu"
            else:
                self.device = settings["device"]

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.settings = settings
            
            # === –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Å–µ–∫—Ü–∏–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
            self.generation = config.get("generation", {
                "max_new_tokens": 1200,
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 50,
                "repetition_penalty": 1.1,
                "no_repeat_ngram_size": 3,
                "do_sample": True,
                "early_stopping": True,
                "pad_token_id": None
            })
            self.performance = config.get("performance", {
                "max_history": 100,
                "context_window": 5,
                "batch_size": 1,
                "gradient_checkpointing": False,
                "memory_efficient_attention": True,
                "compile_mode": "reduce-overhead",
                "max_response_length": 50000,
                "max_context_length": 4096
            })
            self.monitoring = config.get("monitoring", {
                "enable_metrics": True,
                "memory_monitoring": True,
                "performance_tracking": True,
                "error_reporting": True,
                "metrics_retention": 1000,
                "alert_thresholds": {
                    "inference_latency_ms": 5000,
                    "memory_usage_mb": 8192,
                    "min_tokens_per_second": 10
                }
            })
            self.optimization = config.get("optimization", {
                "adaptive_memory": True,
                "auto_device_map": True,
                "memory_cleanup": True,
                "cache_optimization": True,
                "memory_efficient_attention": False
            })
            self.text_processing = config.get("text_processing", {
                "unicode_normalization": "NFC",
                "max_input_length": 5000,
                "sanitize_surrogates": True,
                "encoding_fallback": "utf-8"
            })

            # === –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ chat template system ===
            self.chat_template_system = config.get("chat_template_system", {
                "enabled": True,
                "force_override": False,
                "detection_mode": "smart",
                "fallback_strategy": "generic",
                "validation": {
                    "syntax_check": True,
                    "performance_test": False
                }
            })

            self.chat_templates = config.get("chat_templates", {})

            # === –ó–∞–≥—Ä—É–∑–∫–∞ GGUF –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
            self.gguf_settings = config.get("gguf_settings", {})
            self.gguf_optimization = config.get("gguf_optimization", {})


            # === –í–∞–ª–∏–¥–∞—Ü–∏—è chat template –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
            if self.chat_template_system.get("enabled", True):
                self._validate_chat_template_config()

        except Exception as e:
            # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–∏ –æ—à–∏–±–∫–µ
            self.model_info = {
                "name": "deepseek-ai/deepseek-coder-6.7b-instruct",
                "description": "DeepSeek Coder 6.7B Instruct",
                "size": "6.7B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤",
                "context_size": 4096
            }
            self.settings = {
                "device": "cuda" if torch.cuda.is_available() else "cpu",
                "cache_dir": "./model_cache",
                "dtype": "bfloat16"
            }
            self.device = self.settings["device"]

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
            self.generation = {
                "max_new_tokens": 1200,
                "temperature": 0.7,
                "top_p": 0.95,
                "top_k": 50,
                "repetition_penalty": 1.1,
                "no_repeat_ngram_size": 3,
                "do_sample": True,
                "early_stopping": True,
                "pad_token_id": None
            }
            self.performance = {
                "max_history": 100,
                "context_window": 5,
                "max_response_length": 50000,
                "max_context_length": 4096
            }
            self.monitoring = {
                "enable_metrics": True,
                "performance_tracking": True,
                "metrics_retention": 1000,
                "alert_thresholds": {
                    "inference_latency_ms": 5000,
                    "memory_usage_mb": 8192,
                    "min_tokens_per_second": 10
                }
            }
            self.optimization = {
                "adaptive_memory": True,
                "memory_cleanup": True,
                "memory_efficient_attention": False
            }
            self.text_processing = {
                "unicode_normalization": "NFC",
                "max_input_length": 5000,
                "sanitize_surrogates": True,
                "encoding_fallback": "utf-8"
            }
            self.chat_template_system = {
                "enabled": False, # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º —Å–∏—Å—Ç–µ–º—É —à–∞–±–ª–æ–Ω–æ–≤ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                "force_override": False,
                "detection_mode": "smart",
                "fallback_strategy": "generic"
            }

            self.chat_templates = {} # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å
            self.gguf_settings = {}
            self.gguf_optimization = {}

            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞: {e}")
            self.log_error("load_config", f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞: {e}. –ò—Å–ø–æ–ª—å–∑—É—é –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é.")

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
        self.validate_configuration()


    
    def validate_configuration(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å"""
        validation_errors = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ generation
        gen_config = getattr(self, 'generation', {})
        if gen_config.get('temperature', 0.7) < 0.1 or gen_config.get('temperature', 0.7) > 2.0:
            validation_errors.append("generation.temperature –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0.1, 2.0]")

        if gen_config.get('top_p', 0.95) < 0.1 or gen_config.get('top_p', 0.95) > 1.0:
            validation_errors.append("generation.top_p –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ [0.1, 1.0]")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        perf_config = getattr(self, 'performance', {})
        if perf_config.get('max_context_length', 4096) > 8192:
            validation_errors.append("performance.max_context_length –ø—Ä–µ–≤—ã—à–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ 8192")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ text_processing
        text_config = getattr(self, 'text_processing', {})
        valid_normalizations = ['NFC', 'NFD', 'NFKC', 'NFKD']
        if text_config.get('unicode_normalization', 'NFC') not in valid_normalizations:
            validation_errors.append(f"text_processing.unicode_normalization –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –æ–¥–Ω–æ–π –∏–∑: {valid_normalizations}")

        if validation_errors:
            self.log_error("config_validation", f"Configuration validation failed: {'; '.join(validation_errors)}")
            for error in validation_errors:
                print(f"‚ö†Ô∏è Config Warning: {error}")
        else:
            self.log_system_event("Configuration validation passed")

        return len(validation_errors) == 0


    def _validate_chat_template_config(self):
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ chat template system

        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:
        - –°–∏–Ω—Ç–∞–∫—Å–∏—Å Jinja2 —à–∞–±–ª–æ–Ω–æ–≤
        - –ù–∞–ª–∏—á–∏–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
        - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å stop_tokens
        """
        validation_errors = []

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ —à–∞–±–ª–æ–Ω–∞
            if not self.chat_templates:
                validation_errors.append("chat_templates —Å–µ–∫—Ü–∏—è –ø—É—Å—Ç–∞")
                return validation_errors

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ —à–∞–±–ª–æ–Ω–∞
            for template_name, template_config in self.chat_templates.items():
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
                if 'template' not in template_config:
                    validation_errors.append(f"chat_templates.{template_name}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ 'template'")
                    continue

            # –í–∞–ª–∏–¥–∞—Ü–∏—è Jinja2 —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞
            if self.chat_template_system.get("validation", {}).get("syntax_check", True):
                try:
                    from jinja2 import Template, Environment
                    env = Environment()
                    compiled_template = env.from_string(template_config['template'])

                    # –¢–µ—Å—Ç–æ–≤–∞—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ —Å —Ñ–∏–∫—Ç–∏–≤–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
                    test_messages = [
                        {"role": "system", "content": "Test system message"},
                        {"role": "user", "content": "Test user message"}
                    ]
                    compiled_template.render(messages=test_messages)

                except Exception as jinja_error:
                    validation_errors.append(f"chat_templates.{template_name}: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π Jinja2 —Å–∏–Ω—Ç–∞–∫—Å–∏—Å - {jinja_error}")

            # –í–∞–ª–∏–¥–∞—Ü–∏—è stop_tokens
            stop_tokens = template_config.get("stop_tokens", [])
            if not isinstance(stop_tokens, list):
                validation_errors.append(f"chat_templates.{template_name}: stop_tokens –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ default template
            default_template = self.settings.get("default_chat_template", "generic")
            if default_template not in self.chat_templates:
                validation_errors.append(f"default_chat_template '{default_template}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ chat_templates")

        except Exception as e:
            validation_errors.append(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ chat templates: {e}")

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        if validation_errors:
            for error in validation_errors:
                self.log_error("chat_template_validation", error)
                print(f"‚ö†Ô∏è Chat Template Warning: {error}")
        else:
            self.log_system_event("Chat template configuration validation passed")

        return validation_errors


    def _initialize_fallback_config(self):
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–±–æ—è—Ö"""
        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
        self.device = "cpu"
        self.settings = {
            "device": "cpu",
            "dtype": "float32",
            "cache_dir": "./model_cache"
        }

        self.generation = {
            "max_new_tokens": 1200,
            "temperature": 0.7,
            "top_p": 0.95,
            "do_sample": True,
            "repetition_penalty": 1.1
        }

        self.performance = {
            "max_history": 100,
            "context_window": 5,
            "max_response_length": 50000,
            "max_context_length": 4096
        }

        self.monitoring = {"enable_metrics": False}
        self.optimization = {"adaptive_memory": False}
        self.text_processing = {"unicode_normalization": "NFC"}
        self.gguf_optimization = {}

        # Fallback model info
        self.model_info = {
            "name": "deepseek-ai/deepseek-coder-6.7b-instruct",
            "description": "DeepSeek Coder 6.7B (Fallback mode)",
            "size": "6.7B",
            "context_size": 4096
        }


    def _setup_fallback_logging(self):
        """–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è fallback —Ä–µ–∂–∏–º–∞"""
        try:
            import logging
            self.logger = logging.getLogger('AidaFallback')
            self.logger.setLevel(logging.WARNING)

            # Console logging —Ç–æ–ª—å–∫–æ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.WARNING)
            formatter = logging.Formatter('%(levelname)s: %(message)s')
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)

            self.logger.warning("üîß FALLBACK: –°–∏—Å—Ç–µ–º–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–µ–∂–∏–º–µ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏–∏")
        except Exception:
            self.logger = None  # –ü–æ–ª–Ω–∞—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
            print("‚ö†Ô∏è –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ –≤ fallback —Ä–µ–∂–∏–º–µ")


    def get_display_configuration(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI."""
        return {
                "dtype": self.settings.get("dtype"),
                "quantization": self.settings.get("quantization_type"),
                "max_new_tokens": self.generation.get("max_new_tokens"),
                "model_name": self.model_info.get("name"),
                "device": self.device
        }

    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–≥–µ—Ä –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if not hasattr(self, 'logger'):
            self.logger = None

        if self.logger and self.logger.hasHandlers():
            return


        logs_dir = Path("aida_logs")
        logs_dir.mkdir(exist_ok=True)

        log_filename = f"aida_session_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log"
        log_path = logs_dir / log_filename

        self.logger = logging.getLogger('AidaAssistant')
        self.logger.setLevel(logging.INFO)

        file_handler = logging.FileHandler(log_path, encoding='utf-8')
        file_handler.setLevel(logging.INFO)

        formatter = logging.Formatter(
            '%(asctime)s | %(levelname)s | %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)

        self.logger.addHandler(file_handler)

        self.logger.info("="*80)
        self.logger.info(f"üöÄ –ù–û–í–ê–Ø –°–ï–°–°–ò–Ø –ê–ò–î–´ –ù–ê–ß–ê–õ–ê–°–¨")
        self.logger.info(f"üìÖ –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {self.session_start}")
        self.logger.info(f"üíª –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        self.logger.info(f"ü§ñ –ú–æ–¥–µ–ª—å: {self.model_info['description']}")
        self.logger.info("="*80)

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        self.log_system_event(f"–ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω: {self.config_path}")
        self.log_system_event(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        self.log_system_event(f"–ú–æ–¥–µ–ª—å: {self.model_info['description']}")

    def safe_log_string(self, s):
        """–û—á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–∏ –æ—Ç —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        if not s:
            return ""
        return re.sub(r'[\ud800-\udfff]', '', s)

    def log_action(self, action_type, details):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π"""
        if not hasattr(self, 'logger') or self.logger is None:
            return  # Graceful degradation –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ª–æ–≥–≥–µ—Ä–∞
        try:
            user_info = f"[{self.user_name}]" if self.user_name else "[–ê–Ω–æ–Ω–∏–º–Ω—ã–π]"
            safe_details = self.safe_log_string(str(details))
            log_message = f"üìù {action_type} | {user_info} | {safe_details}"
            self.logger.info(log_message)
        except Exception as e:
            print(f"Logging error: {e}")

    def log_error(self, context, error_message):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫ —Å Unicode-—Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–µ–π"""
        if not hasattr(self, 'logger') or self.logger is None:
            return  # Graceful degradation –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ª–æ–≥–≥–µ—Ä–∞
        try:
            # === –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ ===
            clean_message = self._sanitize_unicode_string(str(error_message))
            clean_context = self._sanitize_unicode_string(str(context))

            log_message = f"‚ùå –û–®–ò–ë–ö–ê [{clean_context}]: {clean_message}"
            self.logger.error(log_message)
        except Exception as e:
            # === Fallback –¥–ª—è –∫–∞—Ç–∞—Å—Ç—Ä–æ—Ñ–∏—á–µ—Å–∫–∏—Ö —Å–±–æ–µ–≤ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ===
            print(f"Critical logging failure: {e}")

    
    def _sanitize_unicode_string(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode"""
        if not isinstance(text, str):
            text = str(text)

        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∫–æ–¥–∏—Ä—É–µ–º –≤ UTF-8, –∑–∞–º–µ–Ω—è—è –ª—é–±—ã–µ –æ—à–∏–±–∫–∏ (–≤–∫–ª—é—á–∞—è —Å—É—Ä—Ä–æ–≥–∞—Ç—ã)
        # –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–∏–º–≤–æ–ª-–∑–∞–º–µ–Ω–∏—Ç–µ–ª—å ''.
        # –ó–∞—Ç–µ–º –¥–µ–∫–æ–¥–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —á–∏—Å—Ç—É—é –∏ –±–µ–∑–æ–ø–∞—Å–Ω—É—é —Å—Ç—Ä–æ–∫—É.
        return text.encode('utf-8', 'replace').decode('utf-8')

    def log_system_event(self, event):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π"""
        if not hasattr(self, 'logger') or self.logger is None:
            return  # Graceful degradation –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –ª–æ–≥–≥–µ—Ä–∞
        self.logger.info(f"üîß –°–ò–°–¢–ï–ú–ê: {event}")

    def introduce_myself(self):
        """–ê–∏–¥–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è –∏ –∑–Ω–∞–∫–æ–º–∏—Ç—Å—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"""
        print(f"\nüíú –ü—Ä–∏–≤–µ—Ç! –ú–µ–Ω—è –∑–æ–≤—É—Ç {self.name}!")
        print(f"ü§ñ –Ø —Ç–≤–æ–π –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –Ω–∞ –±–∞–∑–µ {self.model_info['description']} ({self.model_info['size']})")
        print("üí´ –Ø —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Å—å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∞–Ω–∞–ª–∏–∑–µ, –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–∏ –∫–æ–¥–∞ –∏ –æ–±—É—á–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é.")
        print("üß† –ú–æ—è –º–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –æ–≥—Ä–æ–º–Ω–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∫–æ–¥–∞ –∏ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —è–∑—ã–∫–æ–≤!")

        if not self.user_name:
            name = input("\nüåü –ê –∫–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç? –ú–Ω–µ –ø—Ä–∏—è—Ç–Ω–æ –±—É–¥–µ—Ç –∑–Ω–∞—Ç—å: ").strip()
            if name:
                self.user_name = name
                print(f"\nüòä –ó–∞–º–µ—á–∞—Ç–µ–ª—å–Ω–æ, {self.user_name}! –£ –Ω–∞—Å –±—É–¥–µ—Ç –æ—Ç–ª–∏—á–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞! ü§ù")
            else:
                print("\nüòä –•–æ—Ä–æ—à–æ, –±—É–¥–µ–º —Ä–∞–±–æ—Ç–∞—Ç—å –∞–Ω–æ–Ω–∏–º–Ω–æ! –ì–ª–∞–≤–Ω–æ–µ - –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–¥!")

        return self.user_name

    def get_personalized_greeting(self):
        """–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ"""
        now = datetime.now().hour
        name_part = f", {self.user_name}" if self.user_name else ""

        if 5 <= now < 12:
            return f"üåÖ –î–æ–±—Ä–æ–µ —É—Ç—Ä–æ{name_part}! –ê–∏–¥–∞ –≥–æ—Ç–æ–≤–∞ –∫ –Ω–æ–≤—ã–º –≤—ã–∑–æ–≤–∞–º! ‚òï"
        elif 12 <= now < 18:
            return f"üåû –î–æ–±—Ä—ã–π –¥–µ–Ω—å{name_part}! –í—Ä–µ–º—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∏–Ω–≥–∞! üí™"
        elif 18 <= now < 23:
            return f"üåá –î–æ–±—Ä—ã–π –≤–µ—á–µ—Ä{name_part}! –ó–∞–π–º—ë–º—Å—è –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º–∏ –∑–∞–¥–∞—á–∞–º–∏? üßò"
        else:
            return f"üåô –î–æ–±—Ä–æ–π –Ω–æ—á–∏{name_part}! –ù–æ—á–Ω–æ–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ - –º–æ—è —Å—Ç–∏—Ö–∏—è! üíú"

    def aida_think_aloud(self, task_type):
        """–ê–∏–¥–∞ –¥–µ–ª–∏—Ç—Å—è —Å–≤–æ–∏–º–∏ –º—ã—Å–ª—è–º–∏ –æ –∑–∞–¥–∞—á–µ"""
        thoughts = {
            "code_generation": [
                "ü§î –ò–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è –∑–∞–¥–∞—á–∞! –ü–æ–º–æ–≥—É —Å–æ–∑–¥–∞—Ç—å —ç–ª–µ–≥–∞–Ω—Ç–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ...",
                "üí° –ü—Ä–∏–º–µ–Ω—é –∑–Ω–∞–Ω–∏—è –∏–∑ –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å—Ç—Ä–æ–∫ –∫–æ–¥–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞!",
                "üß† –ò—Å–ø–æ–ª—å–∑—É—é —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏ –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏..."
            ],
            "code_analysis": [
                "üîç –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å –ê–∏–¥–∞ - –Ω–∞–π–¥—É –≤—Å–µ —Ç–æ–Ω–∫–æ—Å—Ç–∏!",
                "üßê –ü—Ä–æ–≤–µ—Ä—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å...",
                "üìã –°–æ—Å—Ç–∞–≤–ª—é –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç—á—ë—Ç!"
            ],
            "code_fixing": [
                "üõ†Ô∏è –ê–∏–¥–∞ –ø–æ–º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –∏ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã!",
                "‚ú® –ù–µ –ø—Ä–æ—Å—Ç–æ –∏—Å–ø—Ä–∞–≤–ª—é, –∞ —É–ª—É—á—à—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É!",
                "üéØ –ü—Ä–∏–º–µ–Ω—é –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞..."
            ],
            "code_explanation": [
                "üìö –û–±—ä—è—Å–Ω—é –∫–æ–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è!",
                "üéì –Ø –∑–Ω–∞—é –º–Ω–æ–∂–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ - –ø–æ–¥–µ–ª—é—Å—å –∑–Ω–∞–Ω–∏—è–º–∏!",
                "üí¨ –†–∞—Å—Å–∫–∞–∂—É –Ω–µ —Ç–æ–ª—å–∫–æ '—á—Ç–æ', –Ω–æ –∏ '–ø–æ—á–µ–º—É' –∏–º–µ–Ω–Ω–æ —Ç–∞–∫..."
            ]
        }

        thought = random.choice(thoughts.get(task_type, ["ü§ñ –ê–∏–¥–∞ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å..."]))
        print(f"\nüí≠ {self.name}: {thought}")

    def load_model(self):
        """
        –ú–æ–¥–µ—Ä–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ 
        –∏ GGUF —Ñ–æ—Ä–º–∞—Ç–∞
        –ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
        """
        if self.model and (self.tokenizer or self.is_gguf):
            return True

        try:
            model_config = self.model_info
            model_format = model_config.get("format", "hf")

            if model_format.lower() == "gguf":
                return self._load_gguf_model()
            else:
                return self._load_hf_model()

        except Exception as e:
            return self._handle_generic_error(e)

    def _load_gguf_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ GGUF —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º llama-cpp-python."""
        self.log_system_event("–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ GGUF –º–æ–¥–µ–ª–∏...")
        model_config = self.model_info
        gguf_config = self.gguf_settings
        gguf_opt_config = self.gguf_optimization
        gguf_path = model_config.get("gguf_path")

        # === –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –µ—Å–ª–∏ —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω ===
        if not gguf_path or not Path(gguf_path).exists():
            print(f"\nüîç GGUF —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏: {gguf_path}")
            print("üöÄ –ü–æ–ø—ã—Ç–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏...")

            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            downloaded_path = self._download_gguf_model_automatically(model_config)
            if downloaded_path:
                gguf_path = downloaded_path
                model_config["gguf_path"] = gguf_path  # –û–±–Ω–æ–≤–ª—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
                print(f"‚úÖ GGUF –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {gguf_path}")
            else:
                self.log_error("gguf_load", f"–ù–µ —É–¥–∞–ª–æ—Å—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–≥—Ä—É–∑–∏—Ç—å GGUF –º–æ–¥–µ–ª—å")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º fallback –∫ HF –º–æ–¥–µ–ª–∏ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
                if model_config.get("fallback_to_hf", False):
                    print("‚ö†Ô∏è –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ HF –≤–µ—Ä—Å–∏—é –º–æ–¥–µ–ª–∏...")
                    model_config["format"] = "hf"  # –ú–µ–Ω—è–µ–º —Ñ–æ—Ä–º–∞—Ç
                    return self._load_hf_model()  # –ó–∞–≥—Ä—É–∂–∞–µ–º HF –≤–µ—Ä—Å–∏—é
                else:
                    return False

        print(f"\nüîÑ –ó–∞–≥—Ä—É–∂–∞—é GGUF –º–æ–¥–µ–ª—å: {model_config['description']}")
        print(f"üìç –ü—É—Ç—å: {gguf_path}")

        try:
            # === –°–æ–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è Llama –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞ ===
            llama_params = {
                "model_path": gguf_path,
                "n_gpu_layers": gguf_config.get("n_gpu_layers", -1), # -1 –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
                "n_ctx": gguf_config.get("n_ctx", 4096),
                "n_batch": gguf_config.get("n_batch", 512),
                "verbose": gguf_config.get("verbose", False),
                "n_threads": gguf_config.get("n_threads"),
                "n_threads_batch": gguf_config.get("n_threads_batch"),
                "mul_mat_q": gguf_config.get("mul_mat_q"),
                "f16_kv": gguf_config.get("f16_kv", True),
                "use_mmap": gguf_config.get("use_mmap", True),
                "use_mlock": gguf_config.get("use_mlock", False),
                "chat_format": gguf_config.get("chat_format", "chatml"),
                "flash_attn": gguf_opt_config.get("flash_attention", False) # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Flash Attention
            }
            # –£–¥–∞–ª—è–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å defaults –∏–∑ llama-cpp-python
            llama_params = {k: v for k, v in llama_params.items() if v is not None}


            self.model = Llama(**llama_params)
            self.is_gguf = True
            self.tokenizer = None  # –î–ª—è GGUF —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤—Å—Ç—Ä–æ–µ–Ω –≤ –æ–±—ä–µ–∫—Ç Llama
            self.model_loaded = True  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏

            print("\n" + "="*70)
            print("ü§ñ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ó–ê–ì–†–£–ñ–ï–ù–ù–û–ô GGUF –ú–û–î–ï–õ–ò")
            print("="*70)
            print(f"üìã –ú–æ–¥–µ–ª—å: {model_config['description']}")
            print(f"‚öôÔ∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç (n_ctx): {gguf_config.get('n_ctx')}")
            print(f"üöÄ –°–ª–æ–µ–≤ –Ω–∞ GPU (n_gpu_layers): {gguf_config.get('n_gpu_layers')}")
            print(f"‚ö° Flash Attention: {'–í–∫–ª—é—á–µ–Ω–æ' if gguf_opt_config.get('flash_attention') else '–í—ã–∫–ª—é—á–µ–Ω–æ'}")
            print("="*70)

            self.log_system_event(f"GGUF –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {gguf_path}")
            self._initialize_generation_manager()
            return True

        except Exception as e:
            self.log_error("gguf_load_error", f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ GGUF –º–æ–¥–µ–ª–∏: {e}")
            return self._handle_generic_error(e)


    def _download_gguf_model_automatically(self, model_config: Dict[str, Any]) -> Optional[str]:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ GGUF –º–æ–¥–µ–ª–∏ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

        –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–≥—Ä—É–∑–∫–∏:
        1. Hugging Face Hub (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å —Ç–∞–º –µ—Å—Ç—å)
        2. –ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ URL (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω gguf_download_url)
        3. –ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫—ç—à–µ

        Returns:
            str: –ü—É—Ç—å –∫ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ None –ø—Ä–∏ –Ω–µ—É–¥–∞—á–µ
        """
        try:
            # === –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ Hugging Face Hub ===
            hf_download_path = self._try_download_gguf_from_hf(model_config)
            if hf_download_path:
                return hf_download_path

            # === –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–æ URL ===
            if model_config.get("gguf_download_url"):
                url_download_path = self._try_download_gguf_from_url(model_config)
                if url_download_path:
                    return url_download_path

            # === –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü–æ–∏—Å–∫ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫—ç—à–µ ===
            cache_path = self._search_gguf_in_cache(model_config)
            if cache_path:
                return cache_path

            self.log_error("gguf_auto_download", "–í—Å–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–µ —É–¥–∞–ª–∏—Å—å")
            return None

        except Exception as e:
            self.log_error("gguf_auto_download", f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return None


    def _try_download_gguf_from_hf(self, model_config: Dict[str, Any]) -> Optional[str]:
        """–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GGUF –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Hugging Face Hub."""
        try:
            from huggingface_hub import hf_hub_download, list_repo_files

            model_name = model_config.get("name", "")
            gguf_filename = model_config.get("gguf_filename", "")

            if not model_name:
                return None

            print(f"üîç –ü–æ–∏—Å–∫ GGUF —Ñ–∞–π–ª–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏: {model_name}")

            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏
            try:
                repo_files = list_repo_files(model_name)
                gguf_files = [f for f in repo_files if f.endswith('.gguf')]

                if not gguf_files:
                    print("‚ö†Ô∏è GGUF —Ñ–∞–π–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏")
                    return None

                # –í—ã–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
                target_filename = gguf_filename if gguf_filename in gguf_files else gguf_files[0]
                print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞: {target_filename}")

                # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª
                cache_dir = self.settings.get("cache_dir", "./model_cache")
                downloaded_path = hf_hub_download(
                    repo_id=model_name,
                    filename=target_filename,
                    cache_dir=cache_dir,
                    resume_download=True,
                    local_files_only=False
                )

                if Path(downloaded_path).exists():
                    print(f"‚úÖ GGUF –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —á–µ—Ä–µ–∑ Hugging Face: {downloaded_path}")
                    self.log_system_event(f"GGUF –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å HF Hub: {target_filename}")
                    return downloaded_path

            except Exception as hf_error:
                self.log_error("hf_gguf_download", f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å HF Hub: {hf_error}")
                return None

        except ImportError:
            print("‚ö†Ô∏è huggingface_hub –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install huggingface_hub")
            return None
        except Exception as e:
            self.log_error("hf_gguf_download", f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ HF –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return None


    def _try_download_gguf_from_url(self, model_config: Dict[str, Any]) -> Optional[str]:
        """–ü—Ä—è–º–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ GGUF –º–æ–¥–µ–ª–∏ –ø–æ URL."""
        try:
            import requests
            from urllib.parse import urlparse

            download_url = model_config.get("gguf_download_url", "")
            if not download_url:
                return None

            print(f"üåê –ó–∞–≥—Ä—É–∑–∫–∞ GGUF –º–æ–¥–µ–ª–∏ –ø–æ URL: {download_url}")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏–∑ URL
            parsed_url = urlparse(download_url)
            filename = Path(parsed_url.path).name
            if not filename.endswith('.gguf'):
                filename = f"{model_config.get('name', 'model').replace('/', '_')}.gguf"

            # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            cache_dir = Path(self.settings.get("cache_dir", "./model_cache"))
            cache_dir.mkdir(parents=True, exist_ok=True)
            target_path = cache_dir / filename

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ —É–∂–µ —Ñ–∞–π–ª
            if target_path.exists():
                print(f"‚úÖ –§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {target_path}")
                return str(target_path)

            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º
            response = requests.get(download_url, stream=True)
            response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))

            with open(target_path, 'wb') as f:
                downloaded = 0
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded += len(chunk)
                        if total_size > 0:
                            progress = (downloaded / total_size) * 100
                            print(f"\rüì• –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {progress:.1f}%", end="", flush=True)

            print(f"\n‚úÖ GGUF –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {target_path}")
            self.log_system_event(f"GGUF –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ø–æ URL: {download_url}")
            return str(target_path)

        except ImportError:
            print("‚ö†Ô∏è requests –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install requests")
            return None
        except Exception as e:
             self.log_error("url_gguf_download", f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ URL: {e}")
             return None


    def _search_gguf_in_cache(self, model_config: Dict[str, Any]) -> Optional[str]:
         """–ü–æ–∏—Å–∫ GGUF –º–æ–¥–µ–ª–∏ –≤ –ª–æ–∫–∞–ª—å–Ω–æ–º –∫—ç—à–µ."""
         try:
             cache_dir = Path(self.settings.get("cache_dir", "./model_cache"))
             model_name = model_config.get("name", "").replace("/", "_")

             if not cache_dir.exists():
                 return None

             # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
             search_patterns = [
                 f"{model_name}*.gguf",
                 f"*{model_name.split('-')[-1]}*.gguf",
                 "*.gguf"
             ]

             for pattern in search_patterns:
                 matches = list(cache_dir.glob(pattern))
                 if matches:
                     found_file = matches[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π
                     print(f"üíæ –ù–∞–π–¥–µ–Ω–∞ GGUF –º–æ–¥–µ–ª—å –≤ –∫—ç—à–µ: {found_file}")
                     self.log_system_event(f"GGUF –º–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫—ç—à–µ: {found_file}")
                     return str(found_file)

             return None

         except Exception as e:
             self.log_error("gguf_cache_search", f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –∫—ç—à–µ: {e}")
             return None


    def _load_hf_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å Hugging Face Hub –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ HF."""
        model_config = self.model_info
        settings = self.settings

        use_local = model_config.get("use_local", False)
        local_path = model_config.get("local_path", "")
        fallback_to_remote = model_config.get("fallback_to_remote", True)
        integrity_check = model_config.get("integrity_check", True)

        # –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å fallback
        model_path = None
        loading_strategy = "unknown"

        if use_local and local_path:
            local_model_path = Path(local_path)
            if self._validate_local_model(local_model_path, integrity_check):
                model_path = str(local_model_path)
                loading_strategy = "local"
                print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å: {model_path}")
            elif fallback_to_remote:
                print("‚ö†Ô∏è –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ —É–¥–∞–ª–µ–Ω–Ω—É—é...")
                model_path = model_config["name"]
                loading_strategy = "remote_fallback"
            else:
                raise FileNotFoundError(f"–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {local_model_path}")
        else:
            model_path = model_config["name"]
            loading_strategy = "remote"

        print(f"\nüîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å: {model_config['description']} ({model_config['size']})")
        print(f"üìç –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {loading_strategy}")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
        print("üî§ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        tokenizer_config = {
            "trust_remote_code": settings.get("trust_remote_code", True),
            "local_files_only": loading_strategy == "local"
        }

        if loading_strategy != "local":
            tokenizer_config["cache_dir"] = settings["cache_dir"]

        self.tokenizer = AutoTokenizer.from_pretrained(model_path, **tokenizer_config)

        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ chat_template
        if self.chat_template_system.get("enabled", True):
            self._setup_intelligent_chat_template()
        else:
            print("üîß Chat template system –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            self.log_system_event("–°–∏—Å—Ç–µ–º–∞ chat template –æ—Ç–∫–ª—é—á–µ–Ω–∞ —á–µ—Ä–µ–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
        model_loading_config = self._prepare_model_config(loading_strategy, model_path)

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        print("üß† –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏...")
        self.model = AutoModelForCausalLM.from_pretrained(model_path, **model_loading_config)

        if self.tokenizer.pad_token is None:
            pad_strategy = self.settings.get("pad_token_strategy", "eos")
            if pad_strategy == "unique":
                self.tokenizer.add_special_tokens({'pad_token': '‚ê¢'})
                self.model.resize_token_embeddings(len(self.tokenizer))
                print("üîß –î–æ–±–∞–≤–ª–µ–Ω —É–Ω–∏–∫–∞–ª—å–Ω—ã–π pad_token –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤")
                self.log_system_event("–£–Ω–∏–∫–∞–ª—å–Ω—ã–π pad_token –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ eos_token")
            else:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                print("üîß –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω pad_token = eos_token")

        # –ü–æ—Å—Ç-–∑–∞–≥—Ä—É–∑–æ—á–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
        self._apply_post_load_optimizations()

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏
        success = self._validate_and_report_metrics()
        if not success:
            self.log_error("model_validation", "–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–æ –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –ø—Ä–æ—à–ª–∞")
            return False

        self.is_gguf = False  # –£–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ GGUF –º–æ–¥–µ–ª—å
        self.model_loaded = True  # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ñ–ª–∞–≥ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
        self._initialize_generation_manager()
        self.log_system_event(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path} | –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {loading_strategy}")
        return True


    def _setup_intelligent_chat_template(self):
        """
        –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ chat templates

        –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è –Ω–∞–ª–∏—á–∏—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ template
        - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ override —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏
        - Graceful fallback –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö
        - Comprehensive logging –¥–ª—è debugging
        """
        try:
            # === –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–∏ ===
            has_native_template = self._detect_native_chat_template()
            model_config = self.model_info
            template_system_config = self.chat_template_system

            # === –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ template ===
            should_apply_custom = self._should_apply_custom_template(
                has_native_template,
                template_system_config
            )

            if should_apply_custom:
                selected_template = self._select_optimal_template()
                self._apply_chat_template(selected_template)

                # === –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ ===
                if template_system_config.get("validation", {}).get("performance_test", False):
                    self._performance_test_template(selected_template)
            else:
                self._configure_native_template_usage()

        except Exception as e:
            self.log_error("intelligent_chat_template", f"Setup failed: {e}")
            self._emergency_fallback_template()


    def _detect_native_chat_template(self) -> bool:
        """
        –î–µ—Ç–µ–∫—Ü–∏—è –Ω–∞–ª–∏—á–∏—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ chat template

        Returns:
            bool: True –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∏–º–µ–µ—Ç –≤–∞–ª–∏–¥–Ω—ã–π –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π template
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞
            if not hasattr(self.tokenizer, 'chat_template'):
                return False

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ template –Ω–µ None –∏ –Ω–µ –ø—É—Å—Ç–æ–π
            if self.tokenizer.chat_template is None or len(str(self.tokenizer.chat_template).strip()) == 0:
                return False

            # –ü–æ–ø—ã—Ç–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ template
            from jinja2 import Template, Environment
            env = Environment()
            template = env.from_string(self.tokenizer.chat_template)

            # –¢–µ—Å—Ç —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            test_messages = [{"role": "user", "content": "test"}]
            rendered = template.render(messages=test_messages)

            if len(rendered.strip()) == 0:
                return False

            self.log_system_event(f"–®–∞–±–ª–æ–Ω —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —á–∞—Ç–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω –∏ –ø—Ä–æ–≤–µ—Ä–µ–Ω")
            return True

        except Exception as e:
            self.log_error("native_template_detection", f"Detection failed: {e}")
            return False


    def _should_apply_custom_template(self, has_native_template: bool, config: dict) -> bool:
        """
        –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ template

        Args:
            has_native_template: –ù–∞–ª–∏—á–∏–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ template
            config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è chat template system

        Returns:
            bool: True –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –∫–∞—Å—Ç–æ–º–Ω—ã–π template
        """
        detection_mode = config.get("detection_mode", "smart")
        force_override = config.get("force_override", False)

        if force_override:
            self.log_system_event("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —à–∞–±–ª–æ–Ω –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ force_override=true")
            return True

        if detection_mode == "smart":
            # –£–º–Ω–∞—è –ª–æ–≥–∏–∫–∞: –ø—Ä–∏–º–µ–Ω—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–π —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç –Ω–∞—Ç–∏–≤–Ω–æ–≥–æ
            decision = not has_native_template
            self.log_system_event(f"Smart detection: custom_template={decision}, native_exists={has_native_template}")
            return decision

        elif detection_mode == "always_custom":
            self.log_system_event("Always custom mode activated")
            return True

        elif detection_mode == "always_native":
            self.log_system_event("Always native mode activated")
            return False

        else:
            self.log_error("template_decision", f"Unknown detection_mode: {detection_mode}")
            return not has_native_template


    def _select_optimal_template(self) -> str:
        """
        –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ template –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏ –º–æ–¥–µ–ª–∏

        Returns:
            str: –ò–º—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ template
        """
        model_config = self.model_info
        settings = self.settings

        # === –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ ===
        model_name = model_config["name"].lower()

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        model_patterns = {
            "starcoder": "starcoder2",
        }
        # –ü–æ–∏—Å–∫ –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
        auto_detected = None
        for pattern, template_name in model_patterns.items():
            if pattern in model_name:
                auto_detected = template_name
                break

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –≤—ã–±–æ—Ä–∞
        selected = (
            auto_detected or                          # 1. Auto-detection
            settings.get("default_chat_template") or  # 2. Explicit config
            self.chat_template_system.get("fallback_strategy", "generic")  # 3. Fallback
        )

        # –í–∞–ª–∏–¥–∞—Ü–∏—è —á—Ç–æ –≤—ã–±—Ä–∞–Ω–Ω—ã–π template —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if selected not in self.chat_templates:
            fallback = self.chat_template_system.get("fallback_strategy", "generic")
            self.log_error("template_selection", f"Template '{selected}' not found, using fallback '{fallback}'")
            selected = fallback

        self.log_system_event(f"Selected chat template: {selected} (auto_detected: {auto_detected})")
        return selected


    def _apply_chat_template(self, template_name: str):
        """
        –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ chat template

        Args:
            template_name: –ò–º—è template –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        try:
            if template_name not in self.chat_templates:
                raise ValueError(f"–®–∞–±–ª–æ–Ω '{template_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")

            template_config = self.chat_templates[template_name]

            # === –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ template ===
            self.tokenizer.chat_template = template_config["template"]

            # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
            self._active_chat_template_config = {
                "name": template_name,
                "stop_tokens": template_config.get("stop_tokens", []),
                "add_generation_prompt": template_config.get("add_generation_prompt", False),
                "strip_whitespace": template_config.get("strip_whitespace", True),
                "max_context_optimization": template_config.get("max_context_optimization", False)
            }

            print(f"üîß –ü—Ä–∏–º–µ–Ω–µ–Ω –∫–∞—Å—Ç–æ–º–Ω—ã–π chat_template: {template_name}")
            self.log_system_event(f"Custom chat template applied: {template_name}")

            # === –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ ===
            if template_config.get("max_context_optimization", False):
                self._optimize_template_for_context()

        except Exception as e:
            self.log_error("template_application", f"Failed to apply template '{template_name}': {e}")
            self._emergency_fallback_template()


    def _configure_native_template_usage(self):
        """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ template"""
        self._active_chat_template_config = {
            "name": "native",
            "stop_tokens": [],
            "add_generation_prompt": True,
            "strip_whitespace": False,
            "max_context_optimization": False
        }

        print("üîß –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π chat_template –º–æ–¥–µ–ª–∏")
        self.log_system_event("Native chat template configured for use")


    def _emergency_fallback_template(self):
        """–≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π fallback template –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É—á–∞–µ–≤"""
        emergency_template = (
            "{% for message in messages %}"
            "{{ message['role']|title }}: {{ message['content']|trim }}\n"
            "{% endfor %}"
            "Assistant: "
        )

        self.tokenizer.chat_template = emergency_template
        self._active_chat_template_config = {
            "name": "emergency_fallback",
            "stop_tokens": ["User:", "System:"],
            "add_generation_prompt": False,
            "strip_whitespace": True,
            "max_context_optimization": False
        }

        print("üîß –ê–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω —ç–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π fallback chat_template")
        self.log_system_event("Emergency fallback chat template activated")


    def _optimize_template_for_context(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è template –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
        self.log_system_event("Template optimized for maximum context utilization")


    def _performance_test_template(self, template_name: str):
        """
        –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ template

        Args:
            template_name: –ò–º—è —Ç–µ—Å—Ç–∏—Ä—É–µ–º–æ–≥–æ template
        """

        try:
            # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            test_messages = [
                {"role": "system", "content": "You are a helpful coding assistant."},
                {"role": "user", "content": "Write a Python function to calculate fibonacci numbers."},
                {"role": "assistant", "content": "Here's a Python function for fibonacci calculation..."}

            ]

            # –ò–∑–º–µ—Ä–µ–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
            start_time = time.time()
            for _ in range(10):  # 10 –∏—Ç–µ—Ä–∞—Ü–∏–π –¥–ª—è —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è
                inputs = self.tokenizer.apply_chat_template(
                    test_messages,
                    add_generation_prompt=self._active_chat_template_config["add_generation_prompt"],
                    return_tensors="pt",
                    truncation=True,
                    max_length=1024
                )

            avg_tokenization_time = (time.time() - start_time) / 10

            # –ú–µ—Ç—Ä–∏–∫–∏
            token_count = inputs.shape[1] if inputs is not None else 0

            performance_metrics = {
                "template_name": template_name,
                "avg_tokenization_time_ms": avg_tokenization_time * 1000,
                "token_count": token_count,
                "tokens_per_second": token_count / avg_tokenization_time if avg_tokenization_time > 0 else 0
            }

            self.log_action("template_performance_test", str(performance_metrics))
            print(f"üìä Template performance: {avg_tokenization_time*1000:.2f}ms, {token_count} tokens")

        except Exception as e:
            self.log_error("template_performance_test", f"Performance test failed: {e}")


    def _initialize_generation_manager(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è generation manager —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        self._generation_manager = EnhancedGenerationManager(
            base_config=self.generation,
            monitoring_config=self.monitoring
        )
        self.log_system_event("Generation manager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _validate_local_model(self, local_path: Path, integrity_check: bool = True) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏"""
        if not local_path.exists():
            return False

        if not integrity_check:
            return True

        # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ ===
        #required_files = ["config.json"]
        model_files = ["pytorch_model.bin", "model.safetensors"]
        tokenizer_files = ["tokenizer.json", "tokenizer_config.json"]

        # === –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ ===
        #missing_required = [f for f in required_files if not (local_path / f).exists()]
        has_model_weights = any((local_path / f).exists() for f in model_files)
        has_tokenizer = any((local_path / f).exists() for f in tokenizer_files)

        if not has_model_weights:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏")
            return False

        if not has_tokenizer:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω—ã —Ñ–∞–π–ª—ã —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞")

        return True

    def _prepare_model_config(self, loading_strategy: str, model_path: str) -> dict:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
        settings = self.settings

        # === –ë–∞–∑–æ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===
        dtype_map = {
            "float32": torch.float32,
            "bfloat16": torch.bfloat16,
            "float16": torch.float16
        }

        config = {
            "trust_remote_code": settings.get("trust_remote_code", True),
            "torch_dtype": dtype_map.get(settings["dtype"], torch.bfloat16),
            "low_cpu_mem_usage": settings.get("low_cpu_mem_usage", True),
            "local_files_only": loading_strategy == "local"
        }

        # === Device mapping ===
        optimization = getattr(self, 'optimization', {})
        if settings.get("device") == "cuda" and torch.cuda.is_available():
            if optimization.get("auto_device_map", True):
                config["device_map"] = "auto"
            else:
                config["device_map"] = None
        else:
            config["device_map"] = None

        # === Cache directory –¥–ª—è —É–¥–∞–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ===
        if loading_strategy != "local":
            config["cache_dir"] = settings["cache_dir"]

        # === –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è ===
        quantization_type = settings.get("quantization_type", None)
        if quantization_type == "8bit":
            config["quantization_config"] = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False
            )
            print("üóúÔ∏è –í–∫–ª—é—á–µ–Ω–∞ 8-–±–∏—Ç–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (BitsAndBytesConfig)")
        elif quantization_type == "4bit":
            config["quantization_config"] = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
            print("üóúÔ∏è –í–∫–ª—é—á–µ–Ω–∞ 4-–±–∏—Ç–Ω–∞—è –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (BitsAndBytesConfig)")

        return config

    def _apply_post_load_optimizations(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Å—Ç-–∑–∞–≥—Ä—É–∑–æ—á–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π"""
        settings = self.settings
        optimization = getattr(self, 'optimization', {})

        # === –†–µ–∂–∏–º eval ===
        self.model.eval()

        # === –ö—ç—à –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è ===
        if optimization.get("cache_optimization", True):
            if hasattr(self.model.config, 'use_cache'):
                self.model.config.use_cache = True

            # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ KV-–∫—ç—à–∞ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.backends.cudnn.benchmark = True

        # === –ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ ===
        if self.device == "cpu" and not hasattr(self.model, 'hf_device_map'):
            self.model = self.model.to(self.device)

        # === PyTorch compilation (—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ) ===
        if settings.get("torch_compile", False):
            try:
                self.model = torch.compile(
                    self.model,
                    mode=getattr(self, 'performance', {}).get('compile_mode', 'reduce-overhead')
                )
                print("‚ö° –í–∫–ª—é—á–µ–Ω–∞ PyTorch compilation")
            except Exception as e:
                print(f"‚ö†Ô∏è PyTorch compilation –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")

        # === Cleanup –ø–∞–º—è—Ç–∏ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ ===
        optimization = getattr(self, 'optimization', {})
        if optimization.get("memory_cleanup", True):
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def _validate_and_report_metrics(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ –æ—Ç—á–µ—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º"""
        try:
            if not self.model or not self.tokenizer:
                print("‚ùå –ú–æ–¥–µ–ª—å –∏–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
                return False

            # === –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏ ===
            total_params = sum(p.numel() for p in self.model.parameters())
            trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
 
            # === –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö ===
            try:
                model_device = next(self.model.parameters()).device
                model_dtype = next(self.model.parameters()).dtype
            except StopIteration:
                print("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
                return False

            # === –û–°–ù–û–í–ù–û–ô –í–´–í–û–î –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò ===
            print("\n" + "="*70)
            print("ü§ñ –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ó–ê–ì–†–£–ñ–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò")
            print("="*70)

            # === –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï –û –ö–í–ê–ù–¢–û–í–ê–ù–ò–ò ===
            if self.settings.get("quantization_type"):
                print(f"‚ö†Ô∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∫–∞–∑–∞–Ω—ã –¥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è. –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –º–µ–Ω—å—à–µ")

                print("-"*70)

            print(f"üìã –ú–æ–¥–µ–ª—å: {self.model_info['description']}")
            print(f"üìä –†–∞–∑–º–µ—Ä: {self.model_info['size']}")
            print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {model_device}")
            print(f"üî£ –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {model_dtype}")
            print(f"‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {total_params:,} (–æ–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,})")
            print(f"üìè –ö–æ–Ω—Ç–µ–∫—Å—Ç: {self.model_info.get('context_size', 'unknown')} —Ç–æ–∫–µ–Ω–æ–≤")

            # === GPU —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ ===
            if torch.cuda.is_available() and "cuda" in str(model_device):
                try:
                    memory_allocated = torch.cuda.memory_allocated() / (1024**3)
                    memory_reserved = torch.cuda.memory_reserved() / (1024**3)
                    memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)

                    print(f"üöÄ GPU –ø–∞–º—è—Ç—å:")
                    print(f"   ‚Ä¢ –í—ã–¥–µ–ª–µ–Ω–æ: {memory_allocated:.2f}GB")
                    print(f"   ‚Ä¢ –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {memory_reserved:.2f}GB")
                    print(f"   ‚Ä¢ –í—Å–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ: {memory_total:.2f}GB")
                    print(f"   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {(memory_allocated/memory_total)*100:.1f}%")
               
                except Exception as gpu_error:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è GPU –º–µ—Ç—Ä–∏–∫: {gpu_error}")

            # === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
            print(f"\nüéõÔ∏è –ü–ê–†–ê–ú–ï–¢–†–´ –ì–ï–ù–ï–†–ê–¶–ò–ò:")
            gen_config = getattr(self, 'generation', {})
            for key, value in gen_config.items():
                print(f"   ‚Ä¢ {key}: {value}")

            print("="*70)

            # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ ===
            self.log_system_event(f"Model validation successful: {self.model_info['description']}")
            self.log_system_event(f"Device: {model_device}, Params: {total_params:,}")

            # === –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω ===
            monitoring = getattr(self, 'monitoring', {})
            if monitoring.get("memory_monitoring", True):
                self._log_memory_usage()

            return True

        except Exception as validation_error:
            error_msg = f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏: {validation_error}"
            print(f"‚ùå {error_msg}")
            self.log_error("model_validation", error_msg)
            return False


    def _log_memory_usage(self):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        if not hasattr(self, 'logger'):
            return

        try:
            if torch.cuda.is_available():
                memory_info = {
                    "allocated_gb": round(torch.cuda.memory_allocated() / (1024**3), 3),
                    "reserved_gb": round(torch.cuda.memory_reserved() / (1024**3), 3),
                    "max_allocated_gb": round(torch.cuda.max_memory_allocated() / (1024**3), 3),
                    "device_count": torch.cuda.device_count(),
                    "current_device": torch.cuda.current_device()
                }
                self.logger.info(f"üìä Enhanced GPU Memory Metrics: {memory_info}")
            else:
                # CPU –ø–∞–º—è—Ç—å
                import psutil
                cpu_memory = psutil.Process().memory_info().rss / (1024**3)
                self.logger.info(f"üìä CPU Memory Usage: {cpu_memory:.3f}GB")
        except Exception as memory_error:
            self.logger.warning(f"Memory monitoring failed: {memory_error}")

    def _handle_file_not_found_error(self, error):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏—è —Ñ–∞–π–ª–æ–≤"""
        error_msg = f"–§–∞–π–ª—ã –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {error}"
        print(f"‚ùå {error_msg}")
        print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ local_path –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
        print("   ‚Ä¢ –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è")
        print("   ‚Ä¢ –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ fallback_to_remote: true")
        self.log_error("load_model_file_not_found", error_msg)
        return False

    def _handle_memory_error(self):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –Ω–µ—Ö–≤–∞—Ç–∫–∏ –ø–∞–º—è—Ç–∏"""
        error_msg = "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏"
        print(f"‚ùå {error_msg}")
        print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:")
        print("   ‚Ä¢ –í–∫–ª—é—á–∏—Ç–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é: load_in_8bit: true")
        print("   ‚Ä¢ –ü–µ—Ä–µ–∫–ª—é—á–∏—Ç–µ—Å—å –Ω–∞ CPU: device: 'cpu'")
        print("   ‚Ä¢ –£–º–µ–Ω—å—à–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏")
        print("   ‚Ä¢ –ó–∞–∫—Ä–æ–π—Ç–µ –¥—Ä—É–≥–∏–µ GPU-–ø—Ä–æ—Ü–µ—Å—Å—ã")
        self.log_error("load_model_memory", error_msg)
        return False

    def _handle_generic_error(self, error):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—â–∏—Ö –æ—à–∏–±–æ–∫"""
        error_msg = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {error}"
        print(f"‚ùå {error_msg}")
        self.log_error("load_model_generic", error_msg)
        return False

    def build_chat_messages(self, current_prompt, task_type="general"):
        """–°–æ–∑–¥–∞—ë—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ chat"""
        system_message = self.system_prompts.get(task_type,
            f"–¢—ã {self.name} - –ø–æ–º–æ—â–Ω–∏–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞. –û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.")

        messages = [{"role": "system", "content": system_message}]

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
        if self.conversation_history:
            for interaction in self.conversation_history[-self.context_window:]:
                messages.append({"role": "user", "content": interaction['task'][:300]})
                messages.append({"role": "assistant", "content": interaction['result'][:500]})

        messages.append({"role": "user", "content": current_prompt})
        return messages

    def save_interaction(
        self, 
        task: str, 
        prompt: str,
        result: str, 
        task_type: str
    ) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é"""
        self.conversation_history.append({
            "timestamp": datetime.now().isoformat(),
            "task_type": task_type,
            "task": task,
            "prompt": prompt,
            "result": result
        })
        self.log_action(
            action_type=task_type,
            details=f"–ó–∞–¥–∞—á–∞: {task[:100]}... | –†–µ–∑—É–ª—å—Ç–∞—Ç: {result[:100]}..."
        )

        if len(self.conversation_history) > self.max_history:
            self.conversation_history = self.conversation_history[-self.max_history:]


    def _prepare_unicode_safe_messages(self, prompt: str, task_type: str) -> List[Dict[str, str]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ Unicode-–±–µ–∑–æ–ø–∞—Å–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –º–æ–¥–µ–ª–∏"""
        try:
            # === –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥—è—â–µ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ ===
            safe_prompt = self._sanitize_unicode_string(prompt)

            # === –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π ===
            messages = self.build_chat_messages(safe_prompt, task_type)

            # === Comprehensive message sanitization ===
            return self._sanitize_chat_messages(messages)

        except Exception as e:
            self.log_error("unicode_safe_messages", f"Failed to prepare safe messages: {e}")
            # === Emergency fallback ===
            return [{"role": "user", "content": "Hello"}]


    def _execute_safe_generation(self, messages: List[Dict[str, str]], params: Dict[str, Any]) -> Optional[str]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π Unicode –æ—à–∏–±–æ–∫"""
        try:
            response = self.model.create_chat_completion(
                messages=messages,
                **params
            )

            if response and 'choices' in response and len(response['choices']) > 0:
                raw_content = response['choices'][0]['message']['content']

                # === Immediate Unicode validation ===
                if not self._is_valid_unicode_string(raw_content):
                    self.log_error("model_output_validation",
                                  "Model generated invalid Unicode content")

                    return None

                return raw_content

            return None

        except UnicodeEncodeError as unicode_error:
            # === Re-raise for higher-level handling ===
            raise unicode_error
        except Exception as e:
            self.log_error("safe_generation", f"Generation failed: {e}")
            return None


    def _validate_and_sanitize_model_output(self, raw_output: str) -> Optional[str]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏"""
        if not raw_output:
            return None

        try:
            # === Multi-stage Unicode validation ===

            # Stage 1: Basic Unicode validation
            if not self._is_valid_unicode_string(raw_output):
                self.log_error("output_validation", "Failed basic Unicode validation")
                raw_output = self._emergency_unicode_cleanup(raw_output)

            # Stage 2: Comprehensive sanitization
            sanitized = self._sanitize_unicode_string(raw_output)

            # Stage 3: Final validation
            if not sanitized or not sanitized.strip():
                self.log_error("output_validation", "Sanitization resulted in empty string")
                return None

            # Stage 4: UTF-8 encoding test
            try:
                sanitized.encode('utf-8')
            except UnicodeEncodeError as e:
                self.log_error("output_validation", f"Final UTF-8 encoding test failed: {e}")
                return self._emergency_unicode_cleanup(sanitized)

            return sanitized.strip()

        except Exception as e:
            self.log_error("output_sanitization", f"Sanitization failed: {e}")
            return self._emergency_unicode_cleanup(raw_output) if raw_output else None


    def _is_valid_unicode_string(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ Unicode —Å—Ç—Ä–æ–∫–∏"""
        if not isinstance(text, str):
            return False
        
        try:
            # === Comprehensive Unicode validation ===

            # Check for surrogate characters
            if re.search(r'[\ud800-\udfff]', text):
                return False

            # Check UTF-8 encodability
            text.encode('utf-8')

            # Check for control characters that might cause issues
            if re.search(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', text):
                # Allows some control chars but flags problematic ones
                pass

            return True

        except (UnicodeEncodeError, UnicodeDecodeError):
            return False
        except Exception:
            return False


    def _emergency_unicode_cleanup(self, text: str) -> str:
        """–≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ Unicode —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é"""
        if not text:
            return ""

        try:
            # === Multi-fallback approach ===

            # Method 1: Surrogate removal with ignore
            cleaned = text.encode('utf-8', errors='ignore').decode('utf-8')

            # Method 2: Control character removal
            cleaned = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', cleaned)

            # Method 3: Explicit surrogate removal
            cleaned = re.sub(r'[\ud800-\udfff]', '', cleaned)

            # Method 4: ASCII fallback if still problematic
            if not cleaned or not self._is_valid_unicode_string(cleaned):
                cleaned = ''.join(char for char in text if ord(char) < 128)

            return cleaned[:2000] if cleaned else "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏"

        except Exception:
            return "‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ Unicode"



    def _emergency_fallback_response(self, original_prompt: str, task_type: str) -> str:
        """–≠–∫—Å—Ç—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∞—Ö Unicode"""
        fallback_responses = {
            "general": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å.",
            "code_generation": "‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–ø—Ä–æ—Å—Ç–∏—Ç—å –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏.",
            "code_fixing": "‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞.",
            "free_mode": "–ò–∑–≤–∏–Ω–∏—Ç–µ, –≤–æ–∑–Ω–∏–∫–ª–∞ –ø—Ä–æ–±–ª–µ–º–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–æ–æ–±—â–µ–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
        }

        return fallback_responses.get(task_type, fallback_responses["general"])


    def _safe_save_interaction(self, task: str, prompt: str, result: str, task_type: str):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å Unicode –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π"""
        try:
            safe_task = self._sanitize_unicode_string(task)
            safe_prompt = self._sanitize_unicode_string(prompt)
            safe_result = self._sanitize_unicode_string(result)

            self.save_interaction(
                task=safe_task,
                prompt=safe_prompt,
                result=safe_result,
                task_type=task_type
            )

        except Exception as e:
            self.log_error("safe_save_interaction", f"Failed to save interaction: {e}")


    # === Enhanced Unicode sanitization with production-grade error handling ===
    def _sanitize_unicode_string(self, text: str) -> str:
        """Production-grade Unicode —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è —Å comprehensive error handling"""
        if not isinstance(text, str):
            text = str(text) if text is not None else ""

        if not text:
            return ""

        try:
            # === Apply text processing configuration ===
            text_config = getattr(self, 'text_processing', {})

            # Input length limitation
            max_input_length = text_config.get("max_input_length", 5000)
            if len(text) > max_input_length:
                text = text[:max_input_length]
                self.log_action("text_truncation", f"Input truncated to {max_input_length} characters")

            # === Multi-stage sanitization ===
            # Stage 1: Surrogate removal
            if text_config.get("sanitize_surrogates", True):
                # More aggressive surrogate handling
                sanitized = re.sub(r'[\ud800-\udfff]', '', text)
                sanitized = sanitized.encode('utf-8', errors='ignore').decode('utf-8')
            else:
                sanitized = text

            # Stage 2: Control character removal
            sanitized = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', sanitized)
            # Stage 3: Unicode normalization
            normalization = text_config.get("unicode_normalization", "NFC")
            try:
                sanitized = unicodedata.normalize(normalization, sanitized)
            except Exception as norm_error:
                self.log_error("unicode_normalization", f"Normalization failed: {norm_error}")
                # Fallback to NFD
                sanitized = unicodedata.normalize("NFD", sanitized)

            # Stage 4: Whitespace cleanup
            sanitized = re.sub(r'\s+', ' ', sanitized).strip()

            # Stage 5: Final length limitation
            final_result = sanitized[:2000]

            # Stage 6: Final validation
            if not self._is_valid_unicode_string(final_result):
                # Emergency ASCII fallback
                final_result = ''.join(char for char in sanitized if ord(char) < 128)[:1000]

            return final_result

        except Exception as sanitization_error:
            self.log_error("unicode_sanitization", f"Comprehensive sanitization failed: {sanitization_error}")

            # === Ultimate fallback chain ===
            try:
                # Fallback 1: Basic encoding/decoding
                return text.encode('utf-8', errors='ignore').decode('utf-8')[:1000]
            except:
                try:
                    # Fallback 2: ASCII only
                    return ''.join(char for char in str(text) if ord(char) < 128)[:1000]
                except:
                    # Fallback 3: Emergency response
                    return "‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞"


    def generate_response(self, prompt: str, task_type: str = "general", max_length: Optional[int] = None, override_params: Optional[Dict[str, Any]] = None) -> str:
        """
        –î–∏—Å–ø–µ—Ç—á–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–∑—ã–≤–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –º–µ—Ç–æ–¥
        –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ñ–æ—Ä–º–∞—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (GGUF –∏–ª–∏ HF).
        """
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫
        if not self.model:
            error_msg = "‚ùå –û—à–∏–±–∫–∞: –ø–æ–ø—ã—Ç–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏."
            self.log_error("generate_response_dispatcher", error_msg)
            return error_msg

        if self.is_gguf:
            # –î–ª—è GGUF –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –±–µ—Ä—É—Ç—Å—è –∏–∑ self.gguf_settings –≤–Ω—É—Ç—Ä–∏ —Å–∞–º–æ–≥–æ –º–µ—Ç–æ–¥–∞
            return self.generate_response_gguf(prompt, task_type, max_length)
        else:
            # –î–ª—è HF-–º–æ–¥–µ–ª–µ–π –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ GenerationManager
            return self.generate_response_hf(prompt, task_type, max_length, override_params)


    def generate_response_hf(self, prompt: str, task_type: str = "general", max_length: Optional[int] = None, override_params: Optional[Dict[str, Any]] = None) -> str:
        """
        –ú–æ–¥–µ—Ä–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π JSON –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        # === –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ===
        if not self.model or not self.tokenizer:
            return "‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."

        if not prompt or not isinstance(prompt, str) or len(prompt.strip()) == 0:
            return "‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."

        # === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è generation manager –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç ===
        if not hasattr(self, '_generation_manager'):
            self._initialize_generation_manager()

        # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ===
        prompt = prompt[:10000]  # Truncation safety
        valid_task_types = {"general", "code_generation", "code_analysis", "code_fixing", "code_explanation", "free_mode"}
        task_type = task_type if task_type in valid_task_types else "general"

        # === Override max_length –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω ===
        runtime_overrides = override_params or {}
        if max_length is not None:
            runtime_overrides["max_new_tokens"] = max(100, min(max_length, 4096))

        try:
            # === Performance monitoring setup ===
            start_time = time.time()
            start_memory = self._get_memory_usage()

            self.aida_think_aloud(task_type)

            # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ chat messages ===
            messages = self.build_chat_messages(prompt, task_type)

            # === Tokenization —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ===
            inputs = self._prepare_model_inputs(messages)

            # === –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
            generation_params = self._generation_manager.get_generation_params(
                task_type=task_type,
                override_params=runtime_overrides
            )

            # === –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
            generation_params.update({
                "eos_token_id": self.tokenizer.eos_token_id,
                "pad_token_id": self.tokenizer.pad_token_id
            })

            # === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è debugging ===
            if getattr(self, 'monitoring', {}).get("enable_metrics", True):
                self.log_action("generation_params", f"Task: {task_type}, Params: {generation_params}")

            # === Model inference —Å error handling ===
            response = self._perform_generation_with_monitoring(inputs, generation_params)

            # === Post-processing –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è ===
            if not response:
                return "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏."

            # === Response truncation safety ===
            max_response_length = getattr(self, 'performance', {}).get('max_response_length', 50000)
            if len(response) > max_response_length:
                response = response[:max_response_length] + "... [–æ—Ç–≤–µ—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏]"

            # === Metrics collection ===
            end_time = time.time()
            end_memory = self._get_memory_usage()

            tokens_generated = len(self.tokenizer.encode(response))
            latency_ms = (end_time - start_time) * 1000

            metrics = GenerationMetrics(
                tokens_generated=tokens_generated,
                latency_ms=latency_ms,
                memory_peak_mb=max(start_memory, end_memory),
                throughput_tokens_per_second=tokens_generated / (end_time - start_time) if end_time > start_time else 0,
                temperature_used=generation_params.get("temperature", 0.7),
                model_device=str(next(self.model.parameters()).device)
            )

            self._generation_manager.record_metrics(metrics)

            # === Performance alerting ===
            self._check_performance_thresholds(metrics)

            # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è ===
            self.save_interaction(
                task=prompt[:200],
                prompt=str(messages)[:300],
                result=response[:400],
                task_type=task_type
            )

            return response

        except KeyboardInterrupt:
            # –≠—Ç–∞ —Å—Ç—Ä–æ–∫–∞ –ø–æ–∑–≤–æ–ª–∏—Ç —Å–∏–≥–Ω–∞–ª—É Ctrl+C –∑–∞–≤–µ—Ä—à–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É
            raise

        except torch.cuda.OutOfMemoryError:
            self.log_error("generation_oom", "GPU OOM during generation")
            return "‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ GPU. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å max_new_tokens."
        except Exception as e:
            error_msg = f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)[:200]}"
            self.log_error("generate_response", str(e))
            return error_msg


    def generate_response_gguf(self, prompt: str, task_type: str = "general", max_length: Optional[int] = None) -> str:
        """
        GGUF-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º llama-cpp-python

        –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
        - –ù–∞—Ç–∏–≤–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ chat templates
        - –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è GGUF
        - –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        """
        if not self.model or not self.is_gguf:
            return "‚ùå GGUF –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."

        if not prompt or len(prompt.strip()) == 0:
            return "‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏."

        try:
            # === Performance monitoring setup ===
            start_time = time.time()
            start_memory = self._get_memory_usage()

            self.aida_think_aloud(task_type)

            # === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π ===
            messages = self.build_chat_messages(prompt, task_type)

            # === GGUF-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ===
            gguf_config = self.gguf_settings

            # === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã, —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Å create_chat_completion ===
            chat_completion_params = {
                "max_tokens": max_length or gguf_config.get("max_tokens", 1200),
                "temperature": gguf_config.get("temp", 0.7),
                "top_p": gguf_config.get("top_p", 0.95),
                "repeat_penalty": gguf_config.get("repeat_penalty", 1.1),
                "seed": gguf_config.get("seed", -1),
                "stream": False
            }

            # === Task-specific adjustments ===
            if task_type == "code_generation":
                chat_completion_params.update({"temperature": 0.3, "top_p": 0.9, "repeat_penalty": 1.2})
            elif task_type == "code_fixing":
                chat_completion_params.update({"temperature": 0.2, "top_p": 0.8})

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º API —á–∞—Ç–∞
            response = self.model.create_chat_completion(
                messages=messages,
                **chat_completion_params
            )


            if response and 'choices' in response and len(response['choices']) > 0:
                raw_result = response['choices'][0]['message']['content']
                # === –û—á–∏—Å—Ç–∫–∞ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏ –æ—Ç Unicode-–æ—à–∏–±–æ–∫ ===
                result = self._sanitize_unicode_string(raw_result)
            else:
                return "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç GGUF –º–æ–¥–µ–ª–∏."

            result = result.strip()
            if not result:
                return "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏."

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ —á—Ç–æ-—Ç–æ –æ—Å—Ç–∞–ª–æ—Å—å
            if not result or not result.strip():
                self.log_error("gguf_generation_empty", f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏ —Å—Ç–∞–ª –ø—É—Å—Ç—ã–º –ø–æ—Å–ª–µ Unicode-–æ—á–∏—Å—Ç–∫–∏. –ò—Å—Ö–æ–¥–Ω—ã–π –æ—Ç–≤–µ—Ç: {raw_result[:200]}")
                return "‚ùå –ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏."

            end_time = time.time()
            tokens_generated = len(result.split())  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            latency_ms = (end_time - start_time) * 1000

            self.save_interaction(
                task=prompt[:200],
                prompt=str(messages)[:300],
                result=result[:400],
                task_type=task_type
            )
            self.log_action("gguf_generation_success",
                           f"Task: {task_type}, Tokens: ~{tokens_generated}, "
                           f"Latency: {latency_ms:.1f}ms")
            return result

        except KeyboardInterrupt:
            # –≠—Ç–∞ —Å—Ç—Ä–æ–∫–∞ –ø–æ–∑–≤–æ–ª–∏—Ç —Å–∏–≥–Ω–∞–ª—É Ctrl+C –∑–∞–≤–µ—Ä—à–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É
            raise

        except Exception as e:
            if isinstance(e, UnicodeEncodeError):
                error_msg = f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è Unicode –≤ GGUF: {str(e)[:200]}. –≠—Ç–æ –Ω–µ –¥–æ–ª–∂–Ω–æ –±—ã–ª–æ –ø—Ä–æ–∏–∑–æ–π—Ç–∏ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π."
            else:
                error_msg = f"‚ùå –û—à–∏–±–∫–∞ GGUF –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)[:200]}"

            self.log_error("generate_response_gguf", str(e))
            return error_msg


    def _determine_gguf_chat_format(self) -> str:
        """
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ chat format –¥–ª—è GGUF –º–æ–¥–µ–ª–∏
        –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        """
        model_name = self.model_info.get("name", "").lower()

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —è–≤–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ > –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ > fallback
        explicit_format = self.gguf_settings.get("chat_format")
        if explicit_format:
            return explicit_format

        # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
        format_mapping = {
            "deepseek": "chatml",
            "starcoder": "chatml",
            "codellama": "llama-2",
            "mistral": "mistral-instruct",
            "phi": "phi-3",
            "qwen": "chatml",
            "llama": "llama-2"
        }

        for pattern, chat_format in format_mapping.items():
            if pattern in model_name:
                self.log_action("gguf_chat_format_detected",
                               f"Auto-detected: {chat_format} for {model_name}")
                return chat_format

        # Fallback
        return "chatml"


    def _format_messages_for_gguf(self, messages: List[Dict[str, str]]) -> str:
        """Fallback —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ø—Ä—è–º–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ GGUF"""
        formatted_parts = []

        for msg in messages:
            role = msg["role"].upper()
            content = msg["content"]
            formatted_parts.append(f"<|{role}|>\n{content}")

        formatted_parts.append("<|ASSISTANT|>\n")
        return "\n".join(formatted_parts)


    def _prepare_model_inputs(self, messages: list) -> torch.Tensor:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏ —Å —É—á–µ—Ç–æ–º performance –Ω–∞—Å—Ç—Ä–æ–µ–∫"""
        performance_config = getattr(self, 'performance', {})

        # === Context size management ===
        max_context = min(
                self.model_info.get("context_size", 4096),
                performance_config.get("max_context_length", 4096)
        )

        # === –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π ===
        validated_messages = self._validate_chat_messages(messages)
        if not validated_messages:
            raise ValueError("–ù–µ—Ç –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ—Å–ª–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")

        # === –ü–†–û–í–ï–†–ö–ê, –í–ö–õ–Æ–ß–ï–ù–´ –õ–ò CHAT TEMPLATES ===
        chat_templates_enabled = self.chat_template_system.get("enabled", False)

        try:
            # –ï—Å–ª–∏ —à–∞–±–ª–æ–Ω—ã –≤–∫–ª—é—á–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º apply_chat_template
            if chat_templates_enabled:
                template_config = getattr(self, '_active_chat_template_config', {})
                inputs = self.tokenizer.apply_chat_template(
                    validated_messages,
                    add_generation_prompt=template_config.get("add_generation_prompt", True),
                    return_tensors="pt",
                    truncation=True,
                    max_length=max_context,
                    padding=False
            )

            # –ï—Å–ª–∏ —à–∞–±–ª–æ–Ω—ã –æ—Ç–∫–ª—é—á–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é
            else:
                combined_text = self._fallback_message_encoding(validated_messages)
                if not combined_text.strip():
                    raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏.")

                inputs = self.tokenizer.encode(
                    combined_text,
                    return_tensors="pt",
                    truncation=True,
                    max_length=max_context,
                    add_special_tokens=True # –í–∞–∂–Ω–æ –¥–ª—è –ø—Ä—è–º–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
                )

                # === –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ ===
                if inputs is None or inputs.nelement() == 0:
                    raise ValueError("–¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Ç–µ–Ω–∑–æ—Ä.")

                return inputs.to(self.model.device)

        except Exception as e:
            # –≠—Ç–æ—Ç –±–ª–æ–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—à–∏–±–∫–∏ –æ–±–æ–∏—Ö –º–µ—Ç–æ–¥–æ–≤
            self.log_error("prepare_inputs_failed", f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –≤—Ö–æ–¥–∞: {e}. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞.")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –±—É–¥–µ—Ç –ø–æ–π–º–∞–Ω–æ –≤—ã—à–µ –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–∞–∫ –æ—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏: {e}")


    def _validate_chat_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """–°—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è chat messages –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è TextEncodeInput –æ—à–∏–±–æ–∫"""
        if not isinstance(messages, list):
            self.log_error("validation_error", "–°–æ–æ–±—â–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º")
            return []

        validated = []
        for msg in messages:
            if not isinstance(msg, dict):
                continue

            role = msg.get("role", "").strip().lower()
            content = msg.get("content", "").strip()

            # === –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–æ–ª–∏ ===
            if role not in {"system", "user", "assistant"}:
                continue

            # === –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ===
            if not content or len(content) == 0:
                continue

            # === –û—á–∏—Å—Ç–∫–∞ –æ—Ç –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ ===
            clean_content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', content)
            if not clean_content.strip():
                continue

            validated.append({
                "role": role,
                "content": clean_content[:2000]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã
            })
        return validated


    def _safe_tokenize_messages(self, messages: List[Dict[str, str]]) -> Optional[torch.Tensor]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        # === –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ===
        sanitized_messages = self._sanitize_chat_messages(messages)
        if not sanitized_messages:
            self.log_error("tokenization_validation", "No valid messages after sanitization")
            sanitized_messages = [{"role": "user", "content": "–ù–∞–ø–∏—à–∏ –ø—Ä–æ—Å—Ç–æ–π –∫–æ–¥ –Ω–∞ Python"}]

        # === –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ===
        tokenization_start = time.time()

        try:
            # === Primary Path: chat_template —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ ===
            template_config = getattr(self, '_active_chat_template_config', {
                "add_generation_prompt": True,
                "strip_whitespace": False
            })

            # === –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω strip_whitespace ===
            processed_messages = sanitized_messages
            if template_config.get("strip_whitespace", False):
                processed_messages = [
                    {
                        "role": msg["role"],
                        "content": msg["content"].strip()
                    }
                    for msg in sanitized_messages
                ]

            inputs = self.tokenizer.apply_chat_template(
                processed_messages,
                add_generation_prompt=template_config.get("add_generation_prompt", True),
                return_tensors="pt",
                truncation=True,
                max_length=2048,
                padding=False
            )

            # === –í–∞–ª–∏–¥–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ ===
            if inputs is not None and inputs.nelement() > 0:
                self._record_tokenization_metrics("chat_template", True, tokenization_start)
                return inputs.to(self.model.device)

        except Exception as primary_error:
            self.log_error("tokenization_primary", f"Chat template failed: {primary_error}")
            self._record_tokenization_metrics("chat_template", False, tokenization_start)

        # === Fallback Strategy 1: Direct encoding —Å —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏–µ–π ===
        try:
            fallback_text = self._fallback_message_encoding(sanitized_messages)
            # === –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è fallback —Ç–µ–∫—Å—Ç–∞ ===
            if not fallback_text or len(fallback_text.strip()) == 0:
                self.log_error("tokenization_fallback", "Empty fallback text generated")
                return None

            # === –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è fallback —Ç–µ–∫—Å—Ç–∞ ===
            clean_fallback_text = self._sanitize_unicode_string(fallback_text)

            inputs = self.tokenizer.encode(
                clean_fallback_text,
                return_tensors="pt",
                truncation=True,
                max_length=2048,
                add_special_tokens=True
            )

            if inputs is not None and inputs.nelement() > 0:
                self.log_action("tokenization_fallback", "Fallback encoding successful")
                self._record_tokenization_metrics("direct_encoding", True, tokenization_start)
                return inputs.to(self.model.device)

        except Exception as fallback_error:
            self.log_error("tokenization_fallback", f"Fallback failed: {fallback_error}")
            self._record_tokenization_metrics("direct_encoding", False, tokenization_start)

        # === Fallback Strategy 2: Minimal prompt –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª—É—á–∞–µ–≤ ===
        try:
            minimal_prompt = self._generate_minimal_prompt(sanitized_messages)
            inputs = self.tokenizer.encode(
                minimal_prompt,
                return_tensors="pt",
                max_length=512,
                truncation=True,
                add_special_tokens=True
            )

            if inputs is not None and inputs.nelement() > 0:
                self.log_action("tokenization_minimal", "Minimal prompt fallback successful")
                self._record_tokenization_metrics("minimal_prompt", True, tokenization_start)
                return inputs.to(self.model.device)

        except Exception as minimal_error:
            self.log_error("tokenization_minimal", f"Minimal fallback failed: {minimal_error}")
            self._record_tokenization_metrics("minimal_prompt", False, tokenization_start)

        # === –ü–æ–ª–Ω—ã–π –ø—Ä–æ–≤–∞–ª - –ø–æ–ª–Ω–æ–µ –ø—Ä–æ—Ç–æ–∫–æ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ ===
        self.log_error("tokenization_total_failure", f"All tokenization strategies failed for {len(messages)} messages")
        return None


    def _sanitize_chat_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """–í—Å–µ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è chat messages —Å Unicode –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        if not isinstance(messages, list):
            return []

        sanitized = []
        valid_roles = {"system", "user", "assistant"}

        for i, msg in enumerate(messages):
            try:
                if not isinstance(msg, dict):
                    continue

                # === –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–æ–ª–∏ ===
                role = str(msg.get("role", "")).strip().lower()
                if role not in valid_roles:
                    self.log_error("message_validation", f"Invalid role '{role}' at index {i}")
                    continue

                # === –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ ===
                content = str(msg.get("content", "")).strip()
                if not content:
                    continue

                # === Unicode —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è ===
                clean_content = self._sanitize_unicode_string(content)
                if not clean_content.strip():
                    continue

                # === –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ===
                if len(clean_content) > 4000:
                    clean_content = clean_content[:4000] + "... [truncated for safety]"

                sanitized.append({
                    "role": role,
                    "content": clean_content
                })
            except Exception as sanitization_error:
                self.log_error("message_sanitization", f"Failed to sanitize message {i}: {sanitization_error}")
                continue

        return sanitized


    def _sanitize_unicode_string(self, text: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è Unicode"""
        if not isinstance(text, str):
            text = str(text)

        try:
            # === –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é text_processing ===
            text_config = getattr(self, 'text_processing', {})

            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            max_input_length = text_config.get("max_input_length", 5000)
            if len(text) > max_input_length:
                text = text[:max_input_length]

            # –£–¥–∞–ª–µ–Ω–∏–µ —Å—É—Ä—Ä–æ–≥–∞—Ç–Ω—ã—Ö –ø–∞—Ä –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
            if text_config.get("sanitize_surrogates", True):
                sanitized = text.encode('utf-8', errors='ignore').decode('utf-8')
            else:
                sanitized = text

            # === –£–ø—Ä–∞–≤–ª—è—é—â–∏–µ —Å–∏–º–≤–æ–ª—ã ===
            sanitized = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f]', '', sanitized)

            # === –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é Unicode –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞ ===
            normalization = text_config.get("unicode_normalization", "NFC")
            sanitized = unicodedata.normalize(normalization, sanitized)

            # === –û—á–∏—Å—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ ===
            sanitized = re.sub(r'\s+', ' ', sanitized).strip()

            return sanitized[:2000]  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

        except Exception as sanitization_error:
            self.log_error("unicode_sanitization", f"Sanitization failed: {sanitization_error}")

            # === Fallback —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
            encoding_fallback = getattr(self, 'text_processing', {}).get('encoding_fallback', 'utf-8')
            try:
                return text.encode(encoding_fallback, errors='ignore').decode(encoding_fallback)[:1000]
            except:
                return ''.join(char for char in text if ord(char) < 128)[:1000]


    def _generate_minimal_prompt(self, messages: List[Dict[str, str]]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è minimal prompt –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö fallback —Å–ª—É—á–∞–µ–≤

        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–≥–¥–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—å.
        –°–æ–∑–¥–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π, –Ω–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç.
        """
        if not messages:
            return "–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."

        # === –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ ===
        last_user_message = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                last_user_message = msg.get("content", "")[:500]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                break

            if not last_user_message:
                return "–ü–æ–º–æ–≥–∏—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —Å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º."

            return f"–í–æ–ø—Ä–æ—Å: {last_user_message}\n–û—Ç–≤–µ—Ç:"


    def _record_tokenization_metrics(self, method: str, success: bool, start_time: float):
        """
        –ó–∞–ø–∏—Å—å –º–µ—Ç—Ä–∏–∫ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –¥–ª—è monitoring –∏ optimization

        –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å –æ–±—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –¥–ª—è:
        - –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        - –ê–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        - Alerting –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å–±–æ—è—Ö
        """
        if not hasattr(self, 'monitoring') or not self.monitoring.get("enable_metrics", True):
            return

        try:
            duration_ms = (time.time() - start_time) * 1000

            metric_data = {
                    "method": method,
                    "success": success,
                    "duration_ms": duration_ms,
                    "timestamp": time.time()
            }

            # === –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π –º–µ—Ç—Ä–∏–∫ ===
            if hasattr(self, '_generation_manager'):
                if not hasattr(self._generation_manager, 'tokenization_metrics'):
                    self._generation_manager.tokenization_metrics = []

                self._generation_manager.tokenization_metrics.append(metric_data)

                # === –ü–æ–ª–∏—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è ===
                if len(self._generation_manager.tokenization_metrics) > 500:
                    self._generation_manager.tokenization_metrics = self._generation_manager.tokenization_metrics[-500:]

                # Logging –¥–ª—è debugging
                self.log_action("tokenization_metrics", f"Method: {method}, Success: {success}, Duration: {duration_ms:.1f}ms")

        except Exception as metrics_error:
            pass
   

    def _fallback_message_encoding(self, messages: List[Dict[str, str]]) -> str:
        """Fallback –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏ —Å–±–æ–µ chat_template"""
        parts = []
        for msg in messages:
            role = msg["role"].upper()
            content = unicodedata.normalize('NFC', msg["content"])
            parts.append(f"[{role}]: {content}")

        return "\n".join(parts)


    def _perform_generation_with_monitoring(self, inputs: torch.Tensor, generation_params: Dict[str, Any]) -> str:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ """
        optimization_config = getattr(self, 'optimization', {})
        performance_config = getattr(self, 'performance', {})

        # === Memory optimization ===
        if optimization_config.get("adaptive_memory", True):
            self._optimize_memory_before_generation()

        # === Gradient checkpointing –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ ===
        if performance_config.get("gradient_checkpointing", False):
            self.model.gradient_checkpointing_enable()

        # === –°–æ–∑–¥–∞–Ω–∏–µ attention_mask –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π ===
        attention_mask = (inputs != self.tokenizer.pad_token_id).long()

        # === –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è stop_tokens –∏–∑ –∞–∫—Ç–∏–≤–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ template ===
        template_config = getattr(self, '_active_chat_template_config', {})
        stop_tokens = template_config.get("stop_tokens", [])
        if stop_tokens:
            # === –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è stop_tokens –≤ token IDs ===
            stop_token_ids = []
            for stop_token in stop_tokens:
                try:
                    token_ids = self.tokenizer.encode(
                        stop_token,
                        add_special_tokens=False,
                        return_tensors=None
                    )
                    if isinstance(token_ids, list):
                        stop_token_ids.extend(token_ids)
                    else:
                        stop_token_ids.extend(token_ids.tolist())
                except Exception as e:
                    self.log_error("stop_token_encoding", f"Failed to encode stop_token '{stop_token}': {e}")
                    continue

            # === –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ generation_params ===
            if stop_token_ids:
                existing_eos = generation_params.get("eos_token_id", self.tokenizer.eos_token_id)
                if isinstance(existing_eos, int):
                    generation_params["eos_token_id"] = [existing_eos] + list(set(stop_token_ids))
                elif isinstance(existing_eos, list):
                    generation_params["eos_token_id"] = existing_eos + list(set(stop_token_ids))
                else:
                    generation_params["eos_token_id"] = list(set(stop_token_ids))

                self.log_action("stop_tokens_configured", f"Added {len(stop_token_ids)} stop tokens")

        # === Enhanced Model inference —Å attention_mask ===
        with torch.no_grad():
            # === Memory efficient attention —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ ===
            if optimization_config.get("memory_efficient_attention", False):
                with torch.backends.cuda.sdp_kernel(
                    enable_flash=True,
                    enable_memory_efficient=True
                ):
                    outputs = self.model.generate(
                        inputs,
                        attention_mask=attention_mask,
                        **generation_params
                    )
            else:
                outputs = self.model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    **generation_params
                )

        # === –û—Ç–∫–ª—é—á–µ–Ω–∏–µ gradient checkpointing –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ===
        if performance_config.get("gradient_checkpointing", False):
            self.model.gradient_checkpointing_disable()

        # === –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ ===
        response = self.tokenizer.decode(
            outputs[0][len(inputs[0]):],
            skip_special_tokens=True
        ).strip()

        # === Post-generation cleanup ===
        if optimization_config.get("memory_cleanup", True):
            self._cleanup_memory_after_generation()

        return response

    def _get_memory_usage(self) -> float:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ –≤ GB"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / (1024 ** 3)
        else:
            return psutil.Process().memory_info().rss / (1024 ** 2)

    def _optimize_memory_before_generation(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def _cleanup_memory_after_generation(self):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    def _check_performance_thresholds(self, metrics: GenerationMetrics):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        monitoring_config = getattr(self, 'monitoring', {})

        if not monitoring_config.get("enable_metrics", True):
            return

        thresholds = monitoring_config.get("alert_thresholds", {})

        # === Latency alerting ===
        max_latency = thresholds.get("inference_latency_ms", 5000)
        if metrics.latency_ms > max_latency:
            self.log_action("performance_alert", f"High latency: {metrics.latency_ms:.1f}ms > {max_latency}ms")

        # === Memory alerting ===
        max_memory_mb = thresholds.get("memory_usage_mb", 8192)
        if metrics.memory_peak_mb > max_memory_mb:
            self.log_action("performance_alert", f"High memory usage: {metrics.memory_peak_mb:.1f}MB > {max_memory_mb}MB")

        # === Throughput monitoring ===
        min_throughput = thresholds.get("min_tokens_per_second", 10)
        if metrics.throughput_tokens_per_second < min_throughput:
            self.log_action("performance_alert", f"Low throughput: {metrics.throughput_tokens_per_second:.1f} < {min_throughput} tokens/sec")


    def update_config_runtime(self, config_section: str, updates: Dict[str, Any]) -> bool:
        """
        Runtime –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –±–µ–∑ –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏

        Args:
            config_section: —Å–µ–∫—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥–∞ ('generation', 'performance', 'monitoring', etc.)
            updates: —Å–ª–æ–≤–∞—Ä—å —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è–º–∏

        Returns:
            bool: —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
        """
        try:
            if not hasattr(self, config_section):
                self.log_error("config_update", f"Unknown config section: {config_section}")
                return False

            current_config = getattr(self, config_section)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            if config_section == "generation":
                for key, value in updates.items():
                    if key == "temperature":
                        updates[key] = max(0.1, min(2.0, float(value)))
                    elif key == "top_p":
                        updates[key] = max(0.1, min(1.0, float(value)))
                    elif key == "max_new_tokens":
                        updates[key] = max(50, min(4096, int(value)))

            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
            current_config.update(updates)

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ generation manager –µ—Å–ª–∏ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            if config_section == "generation" and hasattr(self, '_generation_manager'):
                self._generation_manager.base_generation_config.update(updates)

            self.log_action("config_runtime_update", f"Section: {config_section}, Updates: {updates}")

            return True

        except Exception as e:
            self.log_error("config_runtime_update", f"Failed to update {config_section}: {e}")
            return False


    def generate_code(self, task, language="Python"):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ —Å DeepSeek Coder"""
        language = str(language).strip()

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —è–∑—ã–∫–æ–≤ –¥–ª—è DeepSeek
        supported_languages = {
            "python": "Python", "py": "Python",
            "javascript": "JavaScript", "js": "JavaScript", "node": "JavaScript",
            "typescript": "TypeScript", "ts": "TypeScript",
            "java": "Java",
            "c++": "C++", "cpp": "C++", "cxx": "C++",
            "c": "C",
            "c#": "C#", "csharp": "C#",
            "go": "Go", "golang": "Go",
            "rust": "Rust", "rs": "Rust",
            "php": "PHP",
            "ruby": "Ruby", "rb": "Ruby",
            "swift": "Swift",
            "kotlin": "Kotlin", "kt": "Kotlin",
            "scala": "Scala",
            "html": "HTML",
            "css": "CSS",
            "sql": "SQL",
            "bash": "Bash", "shell": "Shell",
            "powershell": "PowerShell", "ps1": "PowerShell",
            "r": "R",
            "matlab": "MATLAB",
            "lua": "Lua"
        }

        normalized_lang = supported_languages.get(language.lower(), language)
        prompt = f"""–°–æ–∑–¥–∞–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–¥ –Ω–∞ —è–∑—ã–∫–µ {normalized_lang} –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –∑–∞–¥–∞—á–∏:

–ó–ê–î–ê–ß–ê: {task}

–¢–†–ï–ë–û–í–ê–ù–ò–Ø:
- –ù–∞–ø–∏—à–∏ —á–∏—Å—Ç—ã–π, —á–∏—Ç–∞–µ–º—ã–π –∫–æ–¥
- –î–æ–±–∞–≤—å –ø–æ–¥—Ä–æ–±–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –ò—Å–ø–æ–ª—å–∑—É–π –ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ —è–∑—ã–∫–∞ {normalized_lang}
- –û–±—Ä–∞–±–æ—Ç–∞–π –≤–æ–∑–º–æ–∂–Ω—ã–µ –æ—à–∏–±–∫–∏ –∏ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
- –î–æ–±–∞–≤—å –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, –µ—Å–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
- –°–ª–µ–¥—É–π —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
- –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

–ù–∞–ø–∏—à–∏ –≥–æ—Ç–æ–≤—ã–π –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é –∫–æ–¥:"""

        return self.generate_response(prompt, "code_generation", max_length=1200)

    def analyze_code(self, code, language="Python"):
        """–ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ —Å DeepSeek"""
        prompt = f"""–ü—Ä–æ–≤–µ–¥–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ –Ω–∞ —è–∑—ã–∫–µ {language}:

```{language.lower()}
{code[:2000]}
```

–ê–ù–ê–õ–ò–ó –ü–û –ö–†–ò–¢–ï–†–ò–Ø–ú:

1. üîç –°–ò–ù–¢–ê–ö–°–ò–° –ò –û–®–ò–ë–ö–ò:
   - –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
   - –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
   - –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ runtime –æ—à–∏–±–∫–∏

2. üèóÔ∏è –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ò –î–ò–ó–ê–ô–ù:
   - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–¥–∞
   - –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
   - –ú–æ–¥—É–ª—å–Ω–æ—Å—Ç—å –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏

3. ‚ö° –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨:
   - –ê–ª–≥–æ—Ä–∏—Ç–º–∏—á–µ—Å–∫–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
   - –£–∑–∫–∏–µ –º–µ—Å—Ç–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

4. üîí –ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨:
   - –£—è–∑–≤–∏–º–æ—Å—Ç–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏
   - –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫

5. üìù –ö–ê–ß–ï–°–¢–í–û –ö–û–î–ê:
   - –ß–∏—Ç–∞–µ–º–æ—Å—Ç—å –∏ –ø–æ–Ω—è—Ç–Ω–æ—Å—Ç—å
   - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º
   - –ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏

6. üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
   - –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
   - –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã

–î–∞–π –¥–µ—Ç–∞–ª—å–Ω—ã–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑:"""

        result = self.generate_response(prompt, "code_analysis", max_length=1200)
        return result

    def explain_code(self, code, detail_level="–ø–æ–¥—Ä–æ–±–Ω–æ"):
        """–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏"""
        detail_instructions = {
            "–∫—Ä–∞—Ç–∫–æ": "–û–±—ä—è—Å–Ω–∏ –∫—Ä–∞—Ç–∫–æ –æ—Å–Ω–æ–≤–Ω—É—é –∏–¥–µ—é –∏ –ª–æ–≥–∏–∫—É –∫–æ–¥–∞",
            "–ø–æ–¥—Ä–æ–±–Ω–æ": "–î–∞–π –¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ —Å —Ä–∞–∑–±–æ—Ä–æ–º –∫–∞–∂–¥–æ–π –≤–∞–∂–Ω–æ–π —á–∞—Å—Ç–∏",
            "–¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤": "–û–±—ä—è—Å–Ω–∏ –∫–æ–¥ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç—ã–º —è–∑—ã–∫–æ–º –¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–æ–≤ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏",
            "—ç–∫—Å–ø–µ—Ä—Ç–Ω–æ": "–î–∞–π —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ —Å –∞–Ω–∞–ª–∏–∑–æ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π –∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"
        }

        instruction = detail_instructions.get(detail_level.lower(), detail_instructions["–ø–æ–¥—Ä–æ–±–Ω–æ"])

        prompt = f"""{instruction}:

```
{code[:2000]}
```

–°–¢–†–£–ö–¢–£–†–ê –û–ë–™–Ø–°–ù–ï–ù–ò–Ø:

1. üéØ –û–ë–©–ï–ï –ù–ê–ó–ù–ê–ß–ï–ù–ò–ï:
   - –ß—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∞
   - –ö–∞–∫—É—é –ø—Ä–æ–±–ª–µ–º—É —Ä–µ—à–∞–µ—Ç

2. üèóÔ∏è –ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
   - –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
   - –í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ –º–µ–∂–¥—É —á–∞—Å—Ç—è–º–∏

3. üîß –î–ï–¢–ê–õ–¨–ù–´–ô –†–ê–ó–ë–û–†:
   - –ê–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π/–º–µ—Ç–æ–¥–æ–≤
   - –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤
   - –í–∞–∂–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö

4. üí° –ö–õ–Æ–ß–ï–í–´–ï –ö–û–ù–¶–ï–ü–¶–ò–ò:
   - –ò—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
   - –¢–µ—Ö–Ω–∏–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
   - –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

5. üìö –ü–†–ê–ö–¢–ò–ß–ï–°–ö–û–ï –ü–†–ò–ú–ï–ù–ï–ù–ò–ï:
   - –ì–¥–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
   - –í–æ–∑–º–æ–∂–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏
   - –°–≤—è–∑–∞–Ω–Ω—ã–µ —Ç–µ–º—ã –¥–ª—è –∏–∑—É—á–µ–Ω–∏—è

–î–∞–π –ø–æ–Ω—è—Ç–Ω–æ–µ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ:"""

        result = self.generate_response(prompt, "code_explanation", max_length=1200)
        return result

    def fix_code(self, code, language="Python"):
        """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –∫–æ–¥–∞"""
        prompt = f"""–ò—Å–ø—Ä–∞–≤—å –∏ —É–ª—É—á—à–∏ –∫–æ–¥ –Ω–∞ —è–∑—ã–∫–µ {language}:

```{language.lower()}
{code[:2000]}
```

–ó–ê–î–ê–ß–ò –ü–û –ò–°–ü–†–ê–í–õ–ï–ù–ò–Æ:

1. üîß –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –û–®–ò–ë–û–ö:
   - –°–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏
   - –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
   - Runtime –æ—à–∏–±–∫–∏

2. üöÄ –£–õ–£–ß–®–ï–ù–ò–ï –ö–ê–ß–ï–°–¢–í–ê:
   - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
   - –£–ª—É—á—à–µ–Ω–∏–µ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏
   - –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã

3. üõ°Ô∏è –ü–û–í–´–®–ï–ù–ò–ï –ù–ê–î–Å–ñ–ù–û–°–¢–ò:
   - –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏—Å–∫–ª—é—á–µ–Ω–∏–π
   - –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
   - –ü—Ä–æ–≤–µ—Ä–∫–∏ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

4. üìù –°–¢–ê–ù–î–ê–†–¢–´ –ö–û–î–ò–†–û–í–ê–ù–ò–Ø:
   - –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –∫–æ–Ω–≤–µ–Ω—Ü–∏—è–º —è–∑—ã–∫–∞
   - –£–ª—É—á—à–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
   - –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

–†–ï–ó–£–õ–¨–¢–ê–¢:
1. –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–¥
2. –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–Ω–µ—Å—ë–Ω–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è–º–∏
3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞–ª—å–Ω–µ–π—à–µ–º—É —Ä–∞–∑–≤–∏—Ç–∏—é

–í–µ—Ä–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–¥ —Å –ø–æ—è—Å–Ω–µ–Ω–∏—è–º–∏:"""

        result = self.generate_response(prompt, "code_fixing", max_length=1200)
        return result

    def get_statistics(self):
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è DeepSeek –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        if not self.conversation_history:
            return f"üìä {self.name}: –ü–æ–∫–∞ —á—Ç–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –Ω–µ—Ç - —ç—Ç–æ –Ω–∞—à–∞ –ø–µ—Ä–≤–∞—è –≤—Å—Ç—Ä–µ—á–∞!"

        total = len(self.conversation_history)
        task_types = {}

        for interaction in self.conversation_history:
            task_type = interaction["task_type"]
            task_types[task_type] = task_types.get(task_type, 0) + 1

        session_duration = datetime.now() - self.session_start
        hours, remainder = divmod(session_duration.total_seconds(), 3600)
        minutes, _ = divmod(remainder, 60)

        stats = f"""üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã {self.name} (DeepSeek Coder):

üî¢ –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:
  ‚Ä¢ –í—Å–µ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {total}
  ‚Ä¢ –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏: {int(hours)}—á {int(minutes)}–º
  ‚Ä¢ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {self.user_name if self.user_name else '–ê–Ω–æ–Ω–∏–º–Ω—ã–π —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ üòä'}
  ‚Ä¢ –ú–æ–¥–µ–ª—å: {self.model_info['description']}

üìà –¢–∏–ø—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á:"""

        for task_type, count in sorted(task_types.items()):
            percentage = (count / total) * 100
            stats += f"\n  ‚Ä¢ {task_type}: {count} —Ä–∞–∑ ({percentage:.1f}%)"

        stats += f"\n\nüí° –ê–∏–¥–∞ –ø–æ–º–æ–≥–ª–∞ —Ä–µ—à–∏—Ç—å {total} –∑–∞–¥–∞—á –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è!"
        return stats

    def generate_with_chat_template(self, messages, max_length=1200):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ chat"""
        if not self.model or not self.tokenizer:
            return "‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞."

        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            safe_messages = []
            for msg in messages:
                if not isinstance(msg, dict): 
                   continue
                role = str(msg.get("role", "")).lower().strip()
                content = str(msg.get("content", "")).strip()

                if (role in {"system", "user", "assistant"} and
                    content and 
                    content != "###" and
                    len(content) > 0):
                    safe_messages.append({"role": role, "content": content})
            if not safe_messages:
                return "‚ùå –ù–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏"

            print(f"üßπ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {len(safe_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π")

            inputs = self.tokenizer.apply_chat_template(
                safe_messages,
                add_generation_prompt=True,
                return_tensors="pt",
                padding=True,
                truncation=True
            ).to(self.model.device)

            attention_mask = inputs.ne(self.tokenizer.pad_token_id).int()

            with torch.no_grad():
                 outputs = self.model.generate(
                    inputs,
                    attention_mask=attention_mask,
                    max_new_tokens=min(max_length, 512),
                    do_sample=True,
                    top_p=0.95,
                    top_k=50,
                    eos_token_id=self.tokenizer.eos_token_id,
                    pad_token_id=self.tokenizer.eos_token_id
                 )

            response = self.tokenizer.decode(
                outputs[0][len(inputs[0]):],
                skip_special_tokens=True
            ).strip()

            return response if response else "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏"

        except Exception as e:
            return f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (chat template): {e}"

    def run_free_mode(self):
        """–°–≤–æ–±–æ–¥–Ω—ã–π —Ä–µ–∂–∏–º"""
        self.log_action("free_mode", "–ù–∞—á–∞–ª–æ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞")

        print(
            "üß† –í–∫–ª—é—á–µ–Ω —Å–≤–æ–±–æ–¥–Ω—ã–π —Ä–µ–∂–∏–º! –ü—Ä–æ—Å—Ç–æ –æ–±—â–∞–π—Å—è, –∞ –ê–∏–¥–∞ –ø–æ–π–º—ë—Ç, —á—Ç–æ —Ç—ã —Ö–æ—á–µ—à—å.\n"
            "‚úçÔ∏è  –ü–∏—à–∏ –∫–æ–º–∞–Ω–¥—ã –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ: '–æ–±—ä—è—Å–Ω–∏', '–Ω–∞–ø–∏—à–∏ –∫–æ–¥', '—É–ª—É—á—à–∏' –∏ —Ç.–¥.\n"
            "üîö  –ß—Ç–æ–±—ã –≤—ã–π—Ç–∏, –Ω–∞–ø–∏—à–∏ '–≤—ã—Ö–æ–¥' –∏–ª–∏ 'exit'."
        )

        while True:
            try:
                user_input = input("\nüí¨ –¢—ã: ").strip()

                if user_input.lower() in ["–≤—ã—Ö–æ–¥", "exit", "quit", "q"]:
                    print("\nüì¶ –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é...")
                    self.log_action("free_mode_session_complete", "–°–µ—Å—Å–∏—è —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
                    break

                if not user_input:
                    print("‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")
                    continue

                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –¥–∏—Å–ø–µ—Ç—á–µ—Ä –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                response = self.generate_response(user_input, task_type="free_mode")

                print(f"\nü§ñ –ê–∏–¥–∞: {response}")

            except KeyboardInterrupt:
                print("\n\n‚è∏Ô∏è  –ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –í–æ–∑–≤—Ä–∞—Ç –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é...")
                self.log_action("free_mode", "–ü—Ä–µ—Ä—ã–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                break
            except Exception as e:
                error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ —Å–≤–æ–±–æ–¥–Ω–æ–º —Ä–µ–∂–∏–º–µ: {e}"
                print(f"\nüí• {error_msg}")
                self.log_error("free_mode_critical", error_msg)
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É
                continue

    def _log_free_mode_session_stats(self, session_start: float, interactions: int, errors: int):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Å—Å–∏–∏ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞"""
        session_duration = time.time() - session_start
        success_rate = ((interactions - errors) / interactions * 100) if interactions > 0 else 0

        stats_message = (
            f"–°–µ—Å—Å–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ | –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {session_duration:.1f}—Å | "
            f"–í–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π: {interactions} | –û—à–∏–±–æ–∫: {errors} | "
            f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%"
        )
        self.log_action("free_mode_session_complete", stats_message)
        print(f"üìä {stats_message}")


    def get_generation_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è monitoring dashboard"""
        if not hasattr(self, '_generation_manager'):
            return {"error": "Generation manager not initialized"}

        metrics_history = self._generation_manager.metrics_history

        if not metrics_history:
            return {"message": "No generation metrics available"}

        # === Aggregate statistics ===
        total_generations = len(metrics_history)
        avg_latency = sum(m.latency_ms for m in metrics_history) / total_generations
        avg_throughput = sum(m.throughput_tokens_per_second for m in metrics_history) / total_generations
        total_tokens = sum(m.tokens_generated for m in metrics_history)

        return {
                "total_generations": total_generations,
                "average_latency_ms": round(avg_latency, 2),
                "average_throughput_tokens_per_second": round(avg_throughput, 2),
                "total_tokens_generated": total_tokens,
                "peak_memory_mb": max(m.memory_peak_mb for m in metrics_history),
                "model_device": metrics_history[-1].model_device if metrics_history else "unknown"
        }

    def apply_generation_config_updates(self, new_config: Dict[str, Any]):
        """Runtime –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"""
        if hasattr(self, '_generation_manager'):
            # === –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
            validated_config = self._generation_manager._validate_and_normalize_params(new_config)

            # === –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ===
            self._generation_manager.base_generation_config.update(validated_config)

            self.log_action("config_update", f"Generation config updated: {validated_config}")
            return True
        return False


# === –§—É–Ω–∫—Ü–∏–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞ ===

def matrix_rain_line(width=60):
    GREEN = "\033[32m"
    RESET = "\033[0m"
    chars = "01‚ñå‚ñì‚ñí‚ñë‚£ø‚†ø‚°ø‚†õ"
    line = ''.join(random.choice(chars) for _ in range(width))
    print(GREEN + line + RESET)

def aida_loading_animation(theme="default"):
    NEON = "\033[38;5;46m"
    DIM = "\033[2m"
    RESET = "\033[0m"

    steps = [
        "üîå –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ê–∏–¥—ã",
        "üì° –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏",
        "üß† –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏",
        "üîç –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ —Å–∏—Å—Ç–µ–º –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞",
        "üü¢ –ê–∏–¥–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ"
    ]

    phrases_matrix = [
        "üß¨ –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...",
        "‚ö° –ê–∏–¥–∞: —Å–∏—Å—Ç–µ–º—ã —Å—Ç–∞–±–∏–ª—å–Ω—ã.",
        "üëÅÔ∏è –°–∫–∞–Ω–∏—Ä—É—é –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –æ –∫–æ–¥–µ...",
        "üåê –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∫–æ–¥–æ–≤–æ–π –º–∞—Ç—Ä–∏—Ü–µ...",
        "üí° –û–ø—Ç–∏–º–∏–∑–∏—Ä—É—é –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º—ã—à–ª–µ–Ω–∏—è...",
        "üß≠ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é –∏–Ω—Ç—É–∏—Ü–∏—é –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç–∞...",
        "üåÄ –†–∞—Å–ø—É—Ç—ã–≤–∞—é —Å–ª–æ–∂–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏...",
        "üìÅ –ó–∞–≥—Ä—É–∂–∞—é —à–∞–±–ª–æ–Ω—ã –∫–æ–¥–∞...",
        "üîì –ê–∫—Ç–∏–≤–∏—Ä—É—é –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏...",
        "üß† –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É—é –ª–æ–≥–∏–∫—É...",
        "üõ∞Ô∏è –ü–µ—Ä–µ—Ö–æ–∂—É –≤ —Ä–µ–∂–∏–º –ø–æ–ª–Ω–æ–π –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏..."
    ]

    print(NEON + "\n–ó–∞–ø—É—Å–∫ –ê–∏–¥—ã...\n" + RESET)
    time.sleep(1)

    for i, step in enumerate(steps):
        sys.stdout.write(DIM + step + RESET)
        sys.stdout.flush()
        for _ in range(3):
            time.sleep(0.25)
            sys.stdout.write(".")
            sys.stdout.flush()
        print(" ‚úì")

        if theme in {"matrix", "hacker"} and i < 4:
            matrix_rain_line()

        if random.random() < 0.6:
            time.sleep(0.3)
            phrase = random.choice(phrases_matrix)
            print(DIM + "üí¨ –ê–∏–¥–∞: " + phrase + RESET)

        time.sleep(0.3)

    print(NEON + "\n>>> –ê–∏–¥–∞ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω–∞. –ì–æ—Ç–æ–≤ –∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é!\n" + RESET)
    time.sleep(0.8)

def aida_greeting():
    print("="*70)
    print("      ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ")
    print("     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó    ‚ïö‚ïê‚ïê‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó")
    print("     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë       ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë")
    print("     ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë       ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë")
    print("     ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë       ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë")
    print("     ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù       ‚ïö‚ïê‚ïù   ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù")
    print("           ü§ñ  AIDA TA ‚Äî Terminal Assistant üöÄ       ")
    print("="*70)

def show_menu():
    print("‚ï≠" + "‚îÄ" * 67 + "‚ïÆ")
    print("‚îÇ üìã –í—ã–±–µ—Ä–∏ –∑–∞–¥–∞—á—É –¥–ª—è –ê–∏–¥—ã:                                    ‚îÇ")
    print("‚îú" + "‚îÄ" * 67 + "‚î§")
    print("‚îÇ 0. üëã –ü–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –ê–∏–¥–æ–π                                   ‚îÇ")
    print("‚îÇ 1. ‚ú® –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥                                       ‚îÇ")
    print("‚îÇ 2. üîç –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥                                    ‚îÇ")
    print("‚îÇ 3. üõ†Ô∏è –ò—Å–ø—Ä–∞–≤–∏—Ç—å –∏ —É–ª—É—á—à–∏—Ç—å –∫–æ–¥                                ‚îÇ")
    print("‚îÇ 4. üìö –û–±—ä—è—Å–Ω–∏—Ç—å –∫–æ–¥                                           ‚îÇ")
    print("‚îÇ 5. üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã                                       ‚îÇ")
    print("‚îÇ 6. üßπ –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é                                        ‚îÇ")
    print("‚îÇ 7. ‚ùå –ó–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞–±–æ—Ç—É                                        ‚îÇ")
    print("‚îÇ 8. üß™ –¢–µ—Å—Ç chat_template –≤—Ä—É—á–Ω—É—é                              ‚îÇ")
    print("‚îÇ 9. üß† –°–≤–æ–±–æ–¥–Ω—ã–π —Ä–µ–∂–∏–º –æ–±—â–µ–Ω–∏—è —Å –ê–∏–¥–æ–π                         ‚îÇ")
    print("‚îÇ10. üìä –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏                        ‚îÇ")
    print("‚îÇ11. ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏                  ‚îÇ")
    print("‚ï∞" + "‚îÄ" * 67 + "‚ïØ")

def aida_react(choice, assistant):
    responses = {
        "0": "üëã –î–∞–≤–∞–π –∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è! –†–∞—Å—Å–∫–∞–∂—É –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –ê–∏–¥—ã...",
        "1": "‚ú® –ê–∏–¥–∞ —Å–æ–∑–¥–∞—Å—Ç –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥!",
        "2": "üîç –ü—Ä–æ–≤–µ–¥—É –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å DeepSeek Coder...",
        "3": "üõ†Ô∏è –ò—Å–ø—Ä–∞–≤–ª—é –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —É–ª—É—á—à—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É!",
        "4": "üìö –û–±—ä—è—Å–Ω—é –∫–æ–¥ –¥–µ—Ç–∞–ª—å–Ω–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ!",
        "5": "üìä –í–æ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –Ω–∞—à–∏—Ö –¥–æ—Å—Ç–∏–∂–µ–Ω–∏–π üìà",
        "6": "üßπ –ù–∞—á–∏–Ω–∞–µ–º —Å —á–∏—Å—Ç–æ–≥–æ –ª–∏—Å—Ç–∞!",
        "7": f"üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è{', ' + assistant.user_name if assistant.user_name else ''}! –ê–∏–¥–∞ –±—É–¥–µ—Ç —Å–∫—É—á–∞—Ç—å! üíú",
        "8": "üß™ –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º!",
        "9": "",
       "10": "üìä –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.",
       "11": "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏"
    }
    print("\n" + responses.get(choice, "‚ùì –ê–∏–¥–∞ –Ω–µ –ø–æ–Ω—è–ª–∞ –≤—ã–±–æ—Ä. –ü–æ–ø—Ä–æ–±—É–π —Å–Ω–æ–≤–∞.") + "\n")

def main():
    # === –ë–ï–ó–û–ü–ê–°–ù–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–£–¢–ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò ===
    config_path = None

    # 1. –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ (–µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω)
    if len(sys.argv) > 1:
        config_path = sys.argv[1]
        print(f"üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–∞: {config_path}")

    # 2. –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è
    elif 'AIDA_CONFIG_PATH' in os.environ:
        config_path = os.environ['AIDA_CONFIG_PATH']
        print(f"üåç –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è: {config_path}")

    # 3. –õ–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–∫—Ä–∏–ø—Ç–∞
    else:
        script_dir = Path(__file__).parent.absolute()
        local_config = script_dir / 'config.json'
        if local_config.exists():
            config_path = str(local_config)
            print(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ª–æ–∫–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config_path}")
        else:
            print("‚ùå config.json –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–∫—Ä–∏–ø—Ç–∞")
            print(f"üí° –û–∂–∏–¥–∞–µ—Ç—Å—è: {local_config}")
            print("üîß –°–æ–∑–¥–∞–π—Ç–µ config.json –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é AIDA_CONFIG_PATH")
            sys.exit(1)

    try:
        assistant = EnhancedCodeAssistant("config.json")
        try:
            print("="*70)
            aida_loading_animation(theme=AIDA_THEME)
            aida_greeting()
            print("="*70)

            if not assistant.load_model():
                sys.exit("\n‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –í—ã—Ö–æ–¥.")

            print(assistant.get_personalized_greeting())
            print("\nüí° –°–æ–≤–µ—Ç: –í—ã–±–µ—Ä–∏ '0', —á—Ç–æ–±—ã —É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ –æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö –ê–∏–¥—ã!")

            # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—ã—Ö–æ–¥–∞
            def check_exit_command(input_str):
                return input_str.strip().lower() in ["–≤—ã—Ö–æ–¥", "exit", "–º–µ–Ω—é", "q"]

            while True:
                show_menu()
                choice = input("üëâ –í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (0-10): ").strip()
                aida_react(choice, assistant)

                assistant.log_action("menu_selection", f"–í—ã–±—Ä–∞–Ω –ø—É–Ω–∫—Ç –º–µ–Ω—é: {choice}")

                if choice == "0":
                    assistant.introduce_myself()
                    print("\nüöÄ –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:")
                    print("‚Ä¢ üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–¥–∞ –Ω–∞ 20+ —è–∑—ã–∫–∞—Ö –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è")
                    print("‚Ä¢ üîç –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
                    print("‚Ä¢ üõ†Ô∏è –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥")
                    print("‚Ä¢ üìö –û–±—ä—è—Å–Ω–µ–Ω–∏—è –æ—Ç –Ω–æ–≤–∏—á–∫–∞ –¥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞")
                    print("‚Ä¢ üß† –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –ø–∞–º—è—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π")
                    print("‚Ä¢ ‚ö° –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –±–æ–ª—å—à–∏–º–∏ –æ–±—ä—ë–º–∞–º–∏ –∫–æ–¥–∞")

                elif choice == "1":
                    task = input("\nüìù –û–ø–∏—à–∏—Ç–µ –∑–∞–¥–∞—á—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞ (–∏–ª–∏ '–≤—ã—Ö–æ–¥', 'exit' –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –º–µ–Ω—é): ").strip()
                    if check_exit_command(task):
                        print("‚Ü©Ô∏è –í–æ–∑–≤—Ä–∞—Ç –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é...")
                        continue

                    if not task:
                        print("‚ö†Ô∏è –ó–∞–¥–∞—á–∞ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π.")
                        continue

                    assistant.log_action("user_input", f"–ó–∞–¥–∞—á–∞: {task}")

                    print("\nüåü –ê–∏–¥–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —è–∑—ã–∫–∏:")
                    print("Python, JavaScript, TypeScript, Java, C++, C, C#, Go, Rust,")
                    print("PHP, Ruby, Swift, Kotlin, Scala, HTML, CSS, SQL, Bash, R –∏ –¥—Ä—É–≥–∏–µ")

                    lang = input("üíª –£–∫–∞–∂–∏—Ç–µ —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é Python) –∏–ª–∏ '–≤—ã—Ö–æ–¥', 'exit' –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –º–µ–Ω—é): ").strip()
                    if check_exit_command(lang):
                        print("‚Ü©Ô∏è –í–æ–∑–≤—Ä–∞—Ç –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é...")
                        continue
                    if not lang:
                        lang = "Python"

                    assistant.log_action("language_selected", f"–í—ã–±—Ä–∞–Ω —è–∑—ã–∫: {lang}")

                    print(f"\nüîÑ –ê–∏–¥–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–¥ –Ω–∞ {lang}...")
                    result = assistant.generate_code(task, lang)
                    assistant.log_action("response_received", f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
                    print("\n" + "="*70)
                    print(result)
                    print("="*70)

                elif choice in {"2", "3", "4"}:
                    lang = input("üíª –£–∫–∞–∂–∏—Ç–µ —è–∑—ã–∫ –∫–æ–¥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é Python), '–≤—ã—Ö–æ–¥', 'exit' –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –º–µ–Ω—é): ").strip()
                    if check_exit_command(lang):
                        print("‚Ü©Ô∏è –í–æ–∑–≤—Ä–∞—Ç –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é...")
                        continue
                    if not lang:
                        lang = "Python"

                    assistant.log_action("user_input", f"–Ø–∑—ã–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {lang}")

                    print("üìù –í–≤–µ–¥–∏—Ç–µ –∫–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
                    print("–î–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤–≤–æ–¥–∞ –Ω–∞–±–µ—Ä–∏—Ç–µ '###' –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ:")
                    print("–î–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞ –≤ –º–µ–Ω—é –≤–≤–µ–¥–∏—Ç–µ '–≤—ã—Ö–æ–¥' –∏–ª–∏ 'exit' –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ.")

                    lines = []
                    exit_requested = False
                    while True:
                        line = input()
                        if check_exit_command(line):
                           exit_requested = True
                           break
                        if line.strip() == "###":
                            break
                        lines.append(line)

                    if exit_requested:
                        print("‚Ü©Ô∏è –í–æ–∑–≤—Ä–∞—Ç –≤ –≥–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é...")
                        continue

                    code = "\n".join(lines)
                    if not code.strip():
                        print("‚ö†Ô∏è –ö–æ–¥ –Ω–µ –±—ã–ª –≤–≤–µ–¥—ë–Ω.")
                        continue

                    assistant.log_action("code_input", f"–ü–æ–ª—É—á–µ–Ω –∫–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ ({len(code)} —Å–∏–º–≤–æ–ª–æ–≤)")

                    if choice == "2":
                        assistant.log_action("task_start", "–ù–∞—á–∞—Ç –∞–Ω–∞–ª–∏–∑ –∫–æ–¥–∞")

                        print(f"\nüîç –ê–∏–¥–∞ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–¥ –Ω–∞ {lang}...")
                        result = assistant.analyze_code(code, lang)

                        assistant.log_action("response_received", f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result[:100]}")

                    elif choice == "3":
                        assistant.log_action("task_start", "–ù–∞—á–∞—Ç–æ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–¥–∞")

                        print(f"\nüõ†Ô∏è –ê–∏–¥–∞ –∏—Å–ø—Ä–∞–≤–ª—è–µ—Ç –∏ —É–ª—É—á—à–∞–µ—Ç –∫–æ–¥ –Ω–∞ {lang}...")
                        result = assistant.fix_code(code, lang)

                        assistant.log_action("response_received", f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result[:100]}")

                    elif choice == "4":
                        print("üìö –í—ã–±–µ—Ä–∏—Ç–µ —É—Ä–æ–≤–µ–Ω—å –æ–±—ä—è—Å–Ω–µ–Ω–∏—è:")
                        print("1. –ö—Ä–∞—Ç–∫–æ - –æ—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è")
                        print("2. –ü–æ–¥—Ä–æ–±–Ω—ã–π —Ä–∞–∑–±–æ—Ä")
                        print("3. –î–ª—è –Ω–æ–≤–∏—á–∫–æ–≤ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ")
                        print("4. –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑")

                        detail_choice = input("–£—Ä–æ–≤–µ–Ω—å (1-4, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2): ").strip()
                        detail_levels = {"1": "–∫—Ä–∞—Ç–∫–æ", "2": "–ø–æ–¥—Ä–æ–±–Ω–æ", "3": "–¥–ª—è –Ω–æ–≤–∏—á–∫–æ–≤", "4": "—ç–∫—Å–ø–µ—Ä—Ç–Ω–æ"}
                        detail = detail_levels.get(detail_choice, "–ø–æ–¥—Ä–æ–±–Ω–æ")

                        assistant.log_action("task_start", f"–ù–∞—á–∞—Ç–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ (—É—Ä–æ–≤–µ–Ω—å: {detail})")

                        print(f"\nüìö –ê–∏–¥–∞ –æ–±—ä—è—Å–Ω—è–µ—Ç –∫–æ–¥ ({detail})...")
                        result = assistant.explain_code(code, detail)

                        assistant.log_action("response_received", f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result[:100]}")

                    print("\n" + "="*70)
                    print(result)
                    print("="*70)

                elif choice == "5":
                    print("\n" + assistant.get_statistics())

                elif choice == "6":
                    confirm = input("ü§î –¢–æ—á–Ω–æ –æ—á–∏—Å—Ç–∏—Ç—å –≤—Å—é –∏—Å—Ç–æ—Ä–∏—é –¥–µ–π—Å—Ç–≤–∏–π? (–¥–∞/–Ω–µ—Ç): ").strip().lower()
                    if confirm in ["–¥–∞", "yes", "y", "–¥"]:
                        assistant.conversation_history = []
                        print("üßπ –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞. –ê–∏–¥–∞ –≥–æ—Ç–æ–≤–∞ –∫ –Ω–æ–≤—ã–º –∑–∞–¥–∞—á–∞–º!")
                    else:
                        print("üìã –ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

                elif choice == "7":
                    print("üéØ –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ —Å–µ—Å—Å–∏–∏:")
                    if assistant.conversation_history:
                        print(f"‚Ä¢ –ó–∞–¥–∞—á: {len(assistant.conversation_history)}")
                        task_types = {}
                        for interaction in assistant.conversation_history:
                            task_type = interaction["task_type"]
                            task_types[task_type] = task_types.get(task_type, 0) + 1

                        for task_type, count in task_types.items():
                            print(f"‚Ä¢ {task_type}: {count}")
                    else:
                        print("‚Ä¢ –≠—Ç–æ –±—ã–ª–∞ –æ–∑–Ω–∞–∫–æ–º–∏—Ç–µ–ª—å–Ω–∞—è —Å–µ—Å—Å–∏—è")

                    print("\nüöÄ –°–ø–∞—Å–∏–±–æ –∑–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ê–∏–¥—ã!")
                    break

                elif choice == "8":
                    assistant.log_action("chat_template_test", "–ù–∞—á–∞–ª–æ —Ä—É—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è" )
                    print("üß™ –í–≤–æ–¥ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è chat_template. –í–≤–µ–¥–∏—Ç–µ —Ä–æ–ª–∏ –∏ —Ç–µ–∫—Å—Ç—ã.")
                    print("–î–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤–≤–µ–¥–∏—Ç–µ '###' –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–æ–ª–∏.")

                    messages = []
                    while True:
                        role_input = input("üßë –†–æ–ª—å (system/user/assistant –∏–ª–∏ ### –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è): ").strip()
                        role = role_input.lower()
                        if role == "###":
                            break

                        assistant.log_action("chat_template_test", f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {role[:100]}...")
                        if role not in {"system", "user", "assistant"}:
                            print("‚ö†Ô∏è –ù–µ–¥–æ–ø—É—Å—Ç–∏–º–∞—è —Ä–æ–ª—å. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ: system, user –∏–ª–∏ assistant.")
                            continue
                        content = input(f"üì® –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç {role}: ").strip()
                        assistant.log_action("chat_template_test", f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {content[:100]}...")
                        if content == "###":
                            print("‚ÑπÔ∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ '###' –∫–∞–∫ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–æ–æ–±—â–µ–Ω–∏—è ‚Äî –∑–∞–≤–µ—Ä—à–∞–µ–º –≤–≤–æ–¥.")
                            break

                        if not content:
                            print("–°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
                            continue

                        if len(content.strip()) == 0 or content.strip() in ["###", "", "quit", "exit"]:
                           print("–ù–µ–¥–æ–ø—É—Å—Ç–∏–º–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–æ–æ–±—â–µ–Ω–∏—è.")
                           continue

                        messages.append({"role": role, "content": content})
                        print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ: {role} -> {content[:50]}...")

                    if not messages:
                        print("‚ö†Ô∏è –°–æ–æ–±—â–µ–Ω–∏—è –Ω–µ –±—ã–ª–∏ –≤–≤–µ–¥–µ–Ω—ã.")
                        continue

                    assistant.log_action("chat_template_test", f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π)")

                    print("\nüîÑ –ê–∏–¥–∞ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –ø–æ chat_template...")
                    try:
                        result = assistant.generate_with_chat_template(messages)
                        assistant.log_action("chat_template_test", f"–†–µ–∑—É–ª—å—Ç–∞—Ç: {result[:100]}...")
                    except Exception as e:
                        error_msg = f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {str(e)}"
                        print(f"\n‚ùå {error_msg}\n")
                        result = error_msg

                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
                    safe_result = result[:400] if isinstance(result, str) else str(result)[:400]


                    task_desc = f"–¢–µ—Å—Ç chat_template ({len(messages)} —Å–æ–æ–±—â–µ–Ω–∏–π)"
                    assistant.save_interaction(
                            task=task_desc[:200],
                            prompt=str(messages)[:300],
                            result=result[:400],
                            task_type="chat_template_test"
                    )

                    print("\n" + "="*70)
                    print(result)
                    print("="*70)

                elif choice == "9":
                    assistant.run_free_mode()

                elif choice == "10":
                    stats = assistant.get_generation_statistics()
                    print("\n" + "="*70)
                    print("üìä ENTERPRISE –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ì–ï–ù–ï–†–ê–¶–ò–ò:")
                    for key, value in stats.items():
                        print(f"  ‚Ä¢ {key}: {value}")
                    print("="*70)

                elif choice == "11":
                    print("\n‚öôÔ∏è RUNTIME –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò")
                    print("–î–æ—Å—Ç—É–ø–Ω—ã–µ —Å–µ–∫—Ü–∏–∏: generation, performance, monitoring")

                    section = input("–í—ã–±–µ—Ä–∏—Ç–µ —Å–µ–∫—Ü–∏—é –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏: ").strip()
                    if section not in ["generation", "performance", "monitoring"]:
                        print("‚ùå –ù–µ–≤–µ—Ä–Ω–∞—è —Å–µ–∫—Ü–∏—è")
                        continue

                    print(f"\nüìã –¢–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ {section}:")
                    current_config = getattr(assistant, section, {})
                    for key, value in current_config.items():
                        print(f"  ‚Ä¢ {key}: {value}")

                    print("\n–ü—Ä–∏–º–µ—Ä –∏–∑–º–µ–Ω–µ–Ω–∏–π (JSON —Ñ–æ—Ä–º–∞—Ç):")
                    print('{"temperature": 0.8, "max_new_tokens": 1500}')

                    try:
                        updates_str = input("\n–í–≤–µ–¥–∏—Ç–µ JSON —Å –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏: ").strip()
                        if updates_str:
                            import json
                            updates = json.loads(updates_str)

                        if assistant.update_config_runtime(section, updates):
                            print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è {section} —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞")
                            print("–ò–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–∏–º–µ–Ω—è—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫ —Ç–µ–∫—É—â–µ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
                        else:
                            print("‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
                    except json.JSONDecodeError:
                        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π JSON —Ñ–æ—Ä–º–∞—Ç")
                    except Exception as e:
                        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")

                else:
                    print("‚ö†Ô∏è –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 0 –¥–æ 11.")

                input("\nüì± –ù–∞–∂–º–∏—Ç–µ Enter –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è...")
                print("\n" + "="*50 + "\n")

        finally:
          # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–∞–∫—Ä—ã—Ç–∏–µ –ª–æ–≥–≥–µ—Ä–∞
            for handler in assistant.logger.handlers:
                handler.close()
                assistant.logger.removeHandler(handler)

    except (FileNotFoundError, SecurityError, ConfigurationError) as e:
        print(f"‚ùå Configuration Error: {e}")
        print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ config.json —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ AIDA_CONFIG_PATH")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Critical initialization error: {e}")
        sys.exit(1)

# –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≥—Ä–∞–º–º—ã
if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nüëã –ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
    except Exception as e:
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print("üîß –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≥—Ä–∞–º–º—É.")
